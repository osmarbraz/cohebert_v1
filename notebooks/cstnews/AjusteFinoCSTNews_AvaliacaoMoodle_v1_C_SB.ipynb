{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "AjusteFinoCSTNews_AvaliacaoMoodle_v1_C_SB.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "05d880e2577c4024a127698ffb23a089": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b60cf5f18cf64247a83aebc59bbae637",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_363593e3369f4aa29a6619ee336667d2",
              "IPY_MODEL_dfed27c141844087a2a99a24defba724"
            ]
          }
        },
        "b60cf5f18cf64247a83aebc59bbae637": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "363593e3369f4aa29a6619ee336667d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_35ca13ca7fb84b079e400be4e952c3a6",
            "_dom_classes": [],
            "description": "Épocas: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 4,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 4,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1f8f9501087040e89cfe333da96593d2"
          }
        },
        "dfed27c141844087a2a99a24defba724": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_45f1f7b9a78649938c1140ea92123076",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4/4 [24:39&lt;00:00, 369.99s/épocas]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4ec3d62726f545adb2832d4a2cefaf12"
          }
        },
        "35ca13ca7fb84b079e400be4e952c3a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1f8f9501087040e89cfe333da96593d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "45f1f7b9a78649938c1140ea92123076": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4ec3d62726f545adb2832d4a2cefaf12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8e12817ee8fb41579e61a7a540ab39fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_806066de79ad44888f25365559da7e7f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ce703f4a2315407781fa0510d3bfa9fc",
              "IPY_MODEL_20726a3880fa49a889388352f9ab2579"
            ]
          }
        },
        "806066de79ad44888f25365559da7e7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ce703f4a2315407781fa0510d3bfa9fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5681f7e15fca4166abe8351a7963e323",
            "_dom_classes": [],
            "description": "Epoca 1: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2490,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2490,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3393bc00dd3a4207a1eba7ba677bd695"
          }
        },
        "20726a3880fa49a889388352f9ab2579": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_48329bd8b4364dd4b4ada0ae3a314742",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2490/2490 [18:01&lt;00:00,  2.30lotes/s, loss=0.0118]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f8b7b53cafaf42d98490b17cc6b3f79a"
          }
        },
        "5681f7e15fca4166abe8351a7963e323": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3393bc00dd3a4207a1eba7ba677bd695": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "48329bd8b4364dd4b4ada0ae3a314742": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f8b7b53cafaf42d98490b17cc6b3f79a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "605a5715e0c949f4addc2075a10beb7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4e82c681c6584c3ea9324347a43bc954",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9c2714a2629d498e92682a0a80e3b8a9",
              "IPY_MODEL_a678ecded56f41eda4fc731048528cb2"
            ]
          }
        },
        "4e82c681c6584c3ea9324347a43bc954": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9c2714a2629d498e92682a0a80e3b8a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4b1db2c2ebff4eb38af5838b9b933fd0",
            "_dom_classes": [],
            "description": "Epoca 2: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2490,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2490,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ea9c2f669e5d4b7185ed9b5fc76d149c"
          }
        },
        "a678ecded56f41eda4fc731048528cb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_23794afa54414cd1a0864020752668b5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2490/2490 [11:58&lt;00:00,  3.47lotes/s, loss=0.0052]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cb5b671089314d04a30a07b005b4f8d9"
          }
        },
        "4b1db2c2ebff4eb38af5838b9b933fd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ea9c2f669e5d4b7185ed9b5fc76d149c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "23794afa54414cd1a0864020752668b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cb5b671089314d04a30a07b005b4f8d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "91f2ee84ab934faba947f7da420665c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ba50ab217c734a3a8ade7869940a647e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6903a8fd7b254c498fdda91260d3fcd8",
              "IPY_MODEL_326e476df50b4b02ab9f517ef77e04ba"
            ]
          }
        },
        "ba50ab217c734a3a8ade7869940a647e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6903a8fd7b254c498fdda91260d3fcd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2be8edfb90144759ad01c5eecd9505b0",
            "_dom_classes": [],
            "description": "Epoca 3: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2490,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2490,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_22f3a949a6f34e26869b788d8095ced1"
          }
        },
        "326e476df50b4b02ab9f517ef77e04ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b2ee41e865174dab84bfcf85e21b9212",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2490/2490 [05:58&lt;00:00,  6.95lotes/s, loss=0.000207]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a532bd40cc4a47c09ed3b7160bf89f71"
          }
        },
        "2be8edfb90144759ad01c5eecd9505b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "22f3a949a6f34e26869b788d8095ced1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b2ee41e865174dab84bfcf85e21b9212": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a532bd40cc4a47c09ed3b7160bf89f71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7e51d4909c29465fbb8d3e60feb21014": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d18c2a5e9c78404db176640306af4488",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6609331566484112bcbd37cff35e3327",
              "IPY_MODEL_e344fb7d0c574d27933dc6c12fc1eacf"
            ]
          }
        },
        "d18c2a5e9c78404db176640306af4488": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6609331566484112bcbd37cff35e3327": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_56d01ef90987422a8b41bf9a93a13550",
            "_dom_classes": [],
            "description": "Epoca 4: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2490,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2490,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_05efbb334c464173b78c5d029fc0d1f4"
          }
        },
        "e344fb7d0c574d27933dc6c12fc1eacf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_226cfad62541490f991473c4338b3b86",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2490/2490 [06:04&lt;00:00,  6.82lotes/s, loss=0.000184]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a0d58e5c72154c12a685039692d0c7ad"
          }
        },
        "56d01ef90987422a8b41bf9a93a13550": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "05efbb334c464173b78c5d029fc0d1f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "226cfad62541490f991473c4338b3b86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a0d58e5c72154c12a685039692d0c7ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b301d88280c341ff9e75b697fe999806": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0e3c82905f5f42819992ab30df785345",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7889588c61c94a56b6110c86deacbfc0",
              "IPY_MODEL_6501867cc72946a1aeabf9a807cf2e4f"
            ]
          }
        },
        "0e3c82905f5f42819992ab30df785345": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7889588c61c94a56b6110c86deacbfc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5b29f0aa86984a3298cf4ec0d99fd1be",
            "_dom_classes": [],
            "description": "Lotes : 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2805,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2805,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c3086a24b4d44815b38e580d8feab2b2"
          }
        },
        "6501867cc72946a1aeabf9a807cf2e4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c101539c55554771b253ee62dd8f1ee7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2805/2805 [01:32&lt;00:00, 30.22lotes/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9f525155d22d43c9a5a9bd952652217d"
          }
        },
        "5b29f0aa86984a3298cf4ec0d99fd1be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c3086a24b4d44815b38e580d8feab2b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c101539c55554771b253ee62dd8f1ee7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9f525155d22d43c9a5a9bd952652217d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/osmarbraz/coebert/blob/main/AjusteFinoCSTNews_AvaliacaoMoodle_v1_C_SB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JNgd_Bc55AB"
      },
      "source": [
        "#Ajuste fino do conjunto de dados CSTNews usando BERT e validação com os dados do OnlineEduc 1.0\n",
        "\n",
        "\n",
        "Realiza o ajuste do MCL BERT pré-treinado usando o conjunto de dados CSTNews e a avaliação com o conjunto de dados OnlineEduc 1.0.\n",
        "\n",
        "- Realiza o ajuste fino nos dados dos dados CSTNEWS.\n",
        "- Utiliza Lotes Inteligentes para otimizar o tempo de execução de treinamento.\n",
        "- Divide o dataset em 70% para treino e 30% para avaliação.\n",
        "- Salva o modelo ajustado para reaproveitamento,\n",
        "- A seção 2 - parametrização define os argumentos da execução.\n",
        "\n",
        "----------------------------\n",
        "\n",
        "**Link biblioteca Transformers:**\n",
        "https://github.com/huggingface/transformers\n",
        "\n",
        "**Artigo original BERT:**\n",
        "https://arxiv.org/pdf/1506.06724.pdf\n",
        "\n",
        "**Artigo padding dinâmico:**\n",
        "https://towardsdatascience.com/divide-hugging-face-transformers-training-time-by-2-or-more-21bf7129db9q-21bf7129db9e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3G9t8llcrKz"
      },
      "source": [
        "# 1 Preparação do ambiente\n",
        "Preparação do ambiente para execução do notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cW_5CN8En7zl"
      },
      "source": [
        "## 1.1 Tempo inicial de processamento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcTEKloUn-VK"
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "# Marca o tempo de início do processamento\n",
        "inicioProcessamento = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOHCMMDsiyZg",
        "outputId": "1e4e6e61-6a3a-47c4-a4bd-2eb38c57dde7"
      },
      "source": [
        "print(\"  Tempo de início de processamento:  {:} (h:mm:ss)\".format(inicioProcessamento))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Tempo de início de processamento:  1623158860.356765 (h:mm:ss)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOcN8hK-scnt"
      },
      "source": [
        "## 1.2 Funções e classes auxiliares"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1q7fizBnsZFQ"
      },
      "source": [
        "Função auxiliar para formatar o tempo como `hh: mm: ss`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Guy6B4whsZFR"
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def formataTempo(tempo):\n",
        "    '''\n",
        "    Pega a tempo em segundos e retorna uma string hh:mm:ss\n",
        "    '''\n",
        "    # Arredonda para o segundo mais próximo.\n",
        "    tempoArredondado = int(round((tempo)))\n",
        "    \n",
        "    # Formata como hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=tempoArredondado))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFoXtrnnMisv"
      },
      "source": [
        "Calcula a média de uma lista tempo string no formato hh:mm:ss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqS6-M1fqb9V"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "from cmath import rect, phase\n",
        "from math import radians, degrees\n",
        "  \n",
        "def mediaAngulo(deg):\n",
        "    return degrees(phase(sum(rect(1, radians(d)) for d in deg)/len(deg)))\n",
        " \n",
        "def mediaTempo(tempos):\n",
        "    '''\n",
        "    Calcula a média de uma lista de tempo string no formato hh:mm:ss\n",
        "    '''\n",
        "    t = (tempo.split(':') for tempo in tempos)\n",
        "    # Converte para segundos\n",
        "    segundos = ((float(s) + int(m) * 60 + int(h) * 3600) for h, m, s in t)\n",
        "    # Verifica se deu algum dia\n",
        "    dia = 24 * 60 * 60\n",
        "    # Converte para angulos\n",
        "    paraAngulos = [s * 360. / dia for s in segundos]\n",
        "    # Calcula a média dos angulos\n",
        "    mediaComoAngulo = mediaAngulo(paraAngulos)\n",
        "    media_segundos = mediaComoAngulo * dia / 360.\n",
        "    if media_segundos < 0:\n",
        "        media_segundos += dia\n",
        "    # Recupera as horas e os minutos  \n",
        "    h, m = divmod(media_segundos, 3600)\n",
        "    # Recupera os minutos e os segundos\n",
        "    m, s = divmod(m, 60)    \n",
        "    return '{:02d}:{:02d}:{:02d}'.format(int(h), int(m), int(s))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mw8KyNOJJLwf"
      },
      "source": [
        "Calcula a soma de uma lista de tempo string no formato hh:mm:ss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4gekVLwJIcD"
      },
      "source": [
        "def somaTempo(tempos):\n",
        "    '''\n",
        "    Calcula a soma de uma lista de tempo string no formato hh:mm:ss\n",
        "    '''\n",
        "    t = (tempo.split(':') for tempo in tempos)\n",
        "    # Converte para segundos\n",
        "    segundos = ((float(s) + int(m) * 60 + int(h) * 3600) for h, m, s in t)\n",
        "    # Soma os segundos\n",
        "    soma_segundos = sum([s * 1. for s in segundos])\n",
        "    # Recupera as horas e os minutos   \n",
        "    h, m = divmod(soma_segundos, 3600)\n",
        "    # Recupera os minutos e os segundos\n",
        "    m, s = divmod(m, 60)    \n",
        "    return '{:02d}:{:02d}:{:02d}'.format(int(h), int(m), int(s))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nszM7hA_IoKP"
      },
      "source": [
        "Em muitos dos meus loops for (de longa duração), imprimirei atualizações periódicas de progresso. Normalmente, eu escolho o intervalo de atualização manualmente, mas para este Notebook, defini uma função auxiliar para fazer essa escolha para mim :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTrHp0_FgOSg"
      },
      "source": [
        "def obter_intervalo_atualizacao(total_iteracoes, numero_atualizacoes):\n",
        "    '''\n",
        "     Esta função tentará escolher um intervalo de atualização de progresso inteligente\n",
        "     com base na magnitude das iterações totais.\n",
        "\n",
        "     Parâmetros:\n",
        "       `total_iteracoes` - O número de iterações no loop for.\n",
        "       `numero_atualizacoes` - Quantas vezes queremos ver uma atualização sobre o\n",
        "                               curso do loop for.\n",
        "     '''\n",
        "    \n",
        "    # Divida o total de iterações pelo número desejado de atualizações. Provavelmente\n",
        "    # este será um número feio.\n",
        "    intervalo_exato = total_iteracoes / numero_atualizacoes\n",
        "\n",
        "    # A função `arredondar` tem a capacidade de arredondar um número para, por exemplo, o\n",
        "    # milésimo mais próximo: round (intervalo_exato, -3)\n",
        "    #\n",
        "    # Para determinar a magnitude para arredondar, encontre a magnitude do total,\n",
        "    # e então vá uma magnitude abaixo disso.\n",
        "    \n",
        "    # Obtenha a ordem de magnitude do total.\n",
        "    ordem_magnitude = len(str(total_iteracoes)) - 1\n",
        "    \n",
        "    # Nosso intervalo de atualização deve ser arredondado para uma ordem de magnitude menor.\n",
        "    magnitude_arrendonda = ordem_magnitude - 1\n",
        "\n",
        "    # Arredonde para baixo e lance para um int.\n",
        "    intervalo_atualizacao = int(round(intervalo_exato, -magnitude_arrendonda))\n",
        "\n",
        "    # Não permita que o intervalo seja zero!\n",
        "    if intervalo_atualizacao == 0:\n",
        "        intervalo_atualizacao = 1\n",
        "\n",
        "    return intervalo_atualizacao"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVKAapz7RCxk"
      },
      "source": [
        "Classe(ModelArguments) de definição dos parâmetros do modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgmN6RqDRDZS"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Dict, Optional\n",
        "from typing import List\n",
        "\n",
        "@dataclass\n",
        "class ModelArguments:\n",
        "    max_seq_len: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"max seq len\"},\n",
        "    )    \n",
        "    pretrained_model_name_or_path: str = field(\n",
        "        default=\"neuralmind/bert-base-portuguese-cased\",\n",
        "        metadata={\"help\": \"nome do modelo pré-treinado do BERT.\"},\n",
        "    )\n",
        "    do_lower_case: bool = field(\n",
        "        default=False,\n",
        "        metadata={\"help\": \"define se o texto do modelo deve ser todo em minúsculo.\"},\n",
        "    )\n",
        "    num_labels: int = field(\n",
        "        default=2,\n",
        "        metadata={\"help\": \"número de rótulos a ser classificado.\"},\n",
        "    )\n",
        "    output_attentions: bool = field(\n",
        "        default=False,\n",
        "        metadata={\"help\": \"habilita se o modelo retorna os pesos de atenção.\"},\n",
        "    )\n",
        "    output_hidden_states: bool = field(\n",
        "        default=False,\n",
        "        metadata={\"help\": \"habilita gerar as camadas ocultas do modelo.\"},\n",
        "    )\n",
        "    optimizer: str = field(\n",
        "        default=\"AdamW\",\n",
        "        metadata={\"help\": \"otimizador do modelo.\"},\n",
        "    )\n",
        "    use_wandb : bool = field(\n",
        "        default=True,\n",
        "        metadata={\"help\": \"habilita o uso do wandb.\"},\n",
        "    )\n",
        "    salvar_modelo_wandb : bool = field(\n",
        "        default=True,\n",
        "        metadata={\"help\": \"habilita o salvamento do modelo no wandb.\"},\n",
        "    )\n",
        "    salvar_modelo : bool = field(\n",
        "        default=True,\n",
        "        metadata={\"help\": \"habilita o salvamento do modelo.\"},\n",
        "    )\n",
        "    salvar_classificacao : bool = field(\n",
        "        default=False,\n",
        "        metadata={\"help\": \"habilita o salvamento da classificação.\"},\n",
        "    )\n",
        "    salvar_avaliacao : bool = field(\n",
        "        default=True,\n",
        "        metadata={\"help\": \"habilita o salvamento do resultado da avaliação.\"},\n",
        "    )  \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AbhztfwLF6u"
      },
      "source": [
        "Funções auxiliares de arquivos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPcKmSvcLFUo"
      },
      "source": [
        "def carregar(nomeArq):\n",
        "    arq = open(nomeArq, 'r')\n",
        "    \n",
        "    paragrafo = \"\"\n",
        "    for linha in arq:\n",
        "        paragrafo = paragrafo + linha\n",
        "    arq.close()\n",
        "    return paragrafo\n",
        "\n",
        "def carregarLista(nomeArq):\n",
        "    arq = open(nomeArq, 'r')    \n",
        "    seg = []\n",
        "    for linha in arq:\n",
        "      linha = linha.rstrip('\\n')\n",
        "      seg.append(linha)\n",
        "    arq.close()\n",
        "    return seg\n",
        "\n",
        "def salvar(nomeArq,seg):                       \n",
        "    arq = open(nomeArq, 'w')\n",
        "    arq.write(str(seg))\n",
        "    arq.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DnPWIRHfq7V"
      },
      "source": [
        "## 1.3 Tratamento de logs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54St2CZf5lWv"
      },
      "source": [
        "# Biblioteca de logging\n",
        "import logging\n",
        "\n",
        "# Formato da mensagem\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GjYtXcMnSAe"
      },
      "source": [
        "## 1.4 Identificando o ambiente Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMiH0E3OnRa1"
      },
      "source": [
        "# Se estiver executando no Google Colaboratory\n",
        "import sys\n",
        "\n",
        "# Retorna true ou false se estiver no Google Colaboratory\n",
        "IN_COLAB = 'google.colab' in sys.modules"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rceIwWa7UmFZ"
      },
      "source": [
        "## 1.5 Biblioteca de limpeza de tela"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXTEvmuhUmjO"
      },
      "source": [
        "from IPython.display import clear_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJzK1XjCnZak"
      },
      "source": [
        "## 1.6 Conecta ao Google Drive\n",
        "\n",
        "É necessário existir a pasta '/content/drive/MyDrive/Colab Notebooks/Data/CSTNEWS_MD_CV_10/Resultados/' para receber os resutlados do notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dz3RRgR-nZan",
        "outputId": "0c788377-e076-4d8d-f09d-e173d4e684e9"
      },
      "source": [
        "# Monta o Google Drive para esta instância de notebook.\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u66iRrtwMrqy"
      },
      "source": [
        "## 1.7 Instalação do wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQd3BrhvMzZs"
      },
      "source": [
        "Instalação"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejzpgGrFM0-j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d20b87c6-2f65-45e0-9b82-c6e5248888ec"
      },
      "source": [
        "!pip install --upgrade wandb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/48/b199e2b3b341ac842108c5db4956091dd75d961cfa77aceb033e99cac20f/wandb-0.10.31-py2.py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 2.9MB/s \n",
            "\u001b[?25hCollecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 8.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/4a/a54b254f67d8f4052338d54ebe90126f200693440a93ef76d254d581e3ec/sentry_sdk-1.1.0-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 20.4MB/s \n",
            "\u001b[?25hCollecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Collecting pathtools\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/da/6f6224fdfc47dab57881fe20c0d1bc3122be290198ba0bf26a953a045d92/GitPython-3.1.17-py3-none-any.whl (166kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 18.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (57.0.0)\n",
            "Requirement already satisfied, skipping upgrade: urllib3>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (2020.12.5)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 7.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: typing-extensions>=3.7.4.0; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Collecting smmap<5,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: subprocess32, pathtools\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp37-none-any.whl size=6502 sha256=406344acf7dd662a886b7cb403393295f67b22bcda41ddfbfa6129b67b845cbd\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp37-none-any.whl size=8807 sha256=a0c0a4bb8851d665f3189d1a2b2c53c753a90150bf15b6c243652baf203f5cad\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "Successfully built subprocess32 pathtools\n",
            "Installing collected packages: subprocess32, docker-pycreds, sentry-sdk, configparser, pathtools, smmap, gitdb, GitPython, shortuuid, wandb\n",
            "Successfully installed GitPython-3.1.17 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.7 pathtools-0.1.2 sentry-sdk-1.1.0 shortuuid-1.0.1 smmap-4.0.0 subprocess32-3.5.4 wandb-0.10.31\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c7JaP-LM2bW"
      },
      "source": [
        "Login via linha de comando"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOvp48GnMuvM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8d82184-cdf3-42d8-c50b-474aa17416de"
      },
      "source": [
        "!wandb login aded3bc0ea651fff536cc08ba69caf8ac4141cfd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pqa-7WXBAw8q"
      },
      "source": [
        "## 1.8 Instalação BERT da Hugging Face"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCdqJCtQN52l"
      },
      "source": [
        "Instala a interface pytorch para o BERT by Hugging Face. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XeR8Sbz0B5x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6356f9f-8c78-427d-b2b3-b262dd676916"
      },
      "source": [
        "!pip install -U transformers==4.5.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers==4.5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (4.0.1)\n",
            "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (20.9)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 19.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (3.0.12)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 24.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.5.1) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.5.1) (3.4.1)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.5.1) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.1) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.1) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.1) (1.15.0)\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RinFHFesVKis"
      },
      "source": [
        "## 1.9 Colaboratory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPngEboiVbfi"
      },
      "source": [
        "Usando Colab GPU para Treinamento\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjWE6WlvVbfj"
      },
      "source": [
        "Uma GPU pode ser adicionada acessando o menu e selecionando:\n",
        "\n",
        "`Edit -> Notebook Settings -> Hardware accelerator -> (GPU)`\n",
        "\n",
        "Em seguida, execute a célula a seguir para confirmar que a GPU foi detectada."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtaYZmc3Vbfj",
        "outputId": "d7b0285e-8922-4378-9c47-f83a4d65a924"
      },
      "source": [
        "# Importando a biblioteca\n",
        "import tensorflow as tf\n",
        "\n",
        "# Recupera o nome do dispositido da GPU.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# O nome do dispositivo deve ser parecido com o seguinte:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Encontrei GPU em: {}'.format(device_name))\n",
        "else:\n",
        "    print('Dispositivo GPU não encontrado')\n",
        "    #raise SystemError('Dispositivo GPU não encontrado')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encontrei GPU em: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYRrUo2XWa8G"
      },
      "source": [
        "Nome da GPU\n",
        "\n",
        "Para que a torch use a GPU, precisamos identificar e especificar a GPU como o dispositivo. Posteriormente, em nosso ciclo de treinamento, carregaremos dados no dispositivo.\n",
        "\n",
        "Vale a pena observar qual GPU você recebeu. A GPU Tesla V100 é muito mais rápido que as outras GPUs, abaixo uma lista ordenada:\n",
        "- 1o Tesla V100-SXM2-16GB(Pro)\n",
        "- 2o Tesla P100-PCIE-16GB\n",
        "- 3o Tesla T4\n",
        "- 4o Tesla P4 (Não tem memória para execução 4 lotes de treino x 8 lotes de avaliação, somente 2 x 4)\n",
        "- 5o Tesla K80 (Não tem memória para execução 4 lotes de treino x 8 lotes de avaliação, somente 2 x 4)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrjqDO6nWa8J",
        "outputId": "6a2fa15f-7dd5-4c6c-f831-22e458267269"
      },
      "source": [
        "# Importando a biblioteca\n",
        "import torch\n",
        "\n",
        "# Se existe GPU disponível...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Diz ao PyTorch para usar GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('Existem {} GPU(s) disponíveis.'.format(torch.cuda.device_count()))\n",
        "\n",
        "    print('Iremos usar a GPU: {}'.format(torch.cuda.get_device_name(0)))\n",
        "\n",
        "# Se não...\n",
        "else:\n",
        "    print('Sem GPU disponível, usando CPU.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Existem 1 GPU(s) disponíveis.\n",
            "Iremos usar a GPU: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGf59D0yVNx9"
      },
      "source": [
        "Memória\n",
        "\n",
        "Memória disponível no ambiente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iC5-pSAVh7_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e82fe30-7a74-47c2-c147-a0adf251306e"
      },
      "source": [
        "# Importando as bibliotecas.\n",
        "from psutil import virtual_memory\n",
        "\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Seu ambiente de execução tem {: .1f} gigabytes de RAM disponível\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Para habilitar um tempo de execução de RAM alta, selecione menu o ambiente de execução> \"Alterar tipo de tempo de execução\"')\n",
        "  print('e selecione High-RAM. Então, execute novamente está célula')\n",
        "else:\n",
        "  print('Você está usando um ambiente de execução de memória RAM alta!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seu ambiente de execução tem  13.6 gigabytes de RAM disponível\n",
            "\n",
            "Para habilitar um tempo de execução de RAM alta, selecione menu o ambiente de execução> \"Alterar tipo de tempo de execução\"\n",
            "e selecione High-RAM. Então, execute novamente está célula\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giOsAS5v61go"
      },
      "source": [
        "# 2 Parametrização"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJ15-ylRRRdD"
      },
      "source": [
        "# Importando as bibliotecas.\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# Definição dos parâmetros de Treinamento\n",
        "training_args = TrainingArguments(\n",
        "    # AjusteFinoMoodle_v1_C_SB_HT = nome do notebook\n",
        "    # E = número de épocas\n",
        "    # lr = taxa de aprendizagem\n",
        "    # b = lotes de treino e avaliação    \n",
        "    output_dir = 'AjusteFinoCSTNews_AvaliacaoMoodle_v1_C_SB_E_4_lr_1_b_4_8',  \n",
        "    save_steps = 0,    \n",
        "    seed = 42,\n",
        "    num_train_epochs = 4, # Intervalo de valores: 2, 3, 4\n",
        "    learning_rate = 1e-5, # Intervalo de valores: 1e-5, 2e-5, 3e-5, 4e-5, 5e-5 \n",
        "    gradient_accumulation_steps = 1,\n",
        "    per_device_train_batch_size = 4, \n",
        "    per_device_eval_batch_size = 8,        \n",
        "    evaluation_strategy = 'epoch'\n",
        ")\n",
        "\n",
        "# Definição dos parâmetros do Modelo\n",
        "model_args = ModelArguments(     \n",
        "    max_seq_len = 512,\n",
        "    #pretrained_model_name_or_path = \"https://neuralmind-ai.s3.us-east-2.amazonaws.com/nlp/bert-large-portuguese-cased/bert-large-portuguese-cased_pytorch_checkpoint.zip\",\n",
        "    pretrained_model_name_or_path = \"https://neuralmind-ai.s3.us-east-2.amazonaws.com/nlp/bert-base-portuguese-cased/bert-base-portuguese-cased_pytorch_checkpoint.zip\",    \n",
        "    #pretrained_model_name_or_path = 'bert-base-multilingual-cased',\n",
        "    do_lower_case = False,   # default True\n",
        "    num_labels = 2,\n",
        "    output_attentions = False,    # default False\n",
        "    output_hidden_states = False, # default False\n",
        "    optimizer = 'AdamW',\n",
        "    use_wandb = False,\n",
        "    salvar_modelo_wandb = False,    \n",
        "    salvar_modelo = False,\n",
        "    salvar_classificacao = False, # Salva o resultado classificações\n",
        "    salvar_avaliacao = False # Salva o resultado da avaliação das classificações\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aEyME6nxO_V"
      },
      "source": [
        "# Verifica o nome do modelo BERT a ser utilizado\n",
        "MODELO_BERT = 'SEM_MODELO_BERT'\n",
        "if 'neuralmind' in model_args.pretrained_model_name_or_path:\n",
        "  MODELO_BERT = '_BERTimbau'\n",
        "else:\n",
        "  if 'multilingual' in model_args.pretrained_model_name_or_path:\n",
        "    MODELO_BERT = '_BERTmultilingual'\n",
        "\n",
        "# Verifica o tamanho do modelo(default large)\n",
        "TAMANHO_BERT = '_large'\n",
        "if 'base' in model_args.pretrained_model_name_or_path:\n",
        "  TAMANHO_BERT = '_base'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gk9I_DX5coOI"
      },
      "source": [
        "# 3 BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQj2wmKDpkrH"
      },
      "source": [
        "## 3.1 Arquivo do PyTorch Checkpoint\n",
        "\n",
        "Lista de modelos da comunidade:\n",
        "* https://huggingface.co/models\n",
        "\n",
        "Português(https://github.com/neuralmind-ai/portuguese-bert):  \n",
        "* **'neuralmind/bert-base-portuguese-cased'**\n",
        "* **'neuralmind/bert-large-portuguese-cased'**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s88stcJRwbhW"
      },
      "source": [
        "### Função download modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajrTjZzapkrK"
      },
      "source": [
        "def downloadModelo(MODELO):\n",
        "\n",
        "  # Importando as bibliotecas.\n",
        "  import os\n",
        "\n",
        "  # Variável para setar o arquivo.\n",
        "  URL_MODELO = None\n",
        "\n",
        "  if 'http' in MODELO:\n",
        "    URL_MODELO = MODELO\n",
        "\n",
        "  # Se a variável foi setada.\n",
        "  if URL_MODELO:\n",
        "\n",
        "    # Diretório descompactação.\n",
        "    DIRETORIO_MODELO = '/content/modelo'\n",
        "\n",
        "    # Recupera o nome do arquivo do modelo da url.\n",
        "    arquivo = URL_MODELO.split(\"/\")[-1]\n",
        "\n",
        "    # Nome do arquivo do vocabulário.\n",
        "    arquivo_vocab = \"vocab.txt\"\n",
        "\n",
        "    # Caminho do arquivo na url.\n",
        "    caminho = URL_MODELO[0:len(URL_MODELO)-len(arquivo)]\n",
        "\n",
        "    # Verifica se a pasta de descompactação existe na pasta corrente\n",
        "    if os.path.exists(DIRETORIO_MODELO):\n",
        "      print(\"Apagando diretório existente do modelo!\")\n",
        "      # Apaga a pasta e os arquivos existentes\n",
        "      !rm -rf $DIRETORIO_MODELO  \n",
        "\n",
        "    # Baixa o arquivo do modelo.\n",
        "    !wget $URL_MODELO\n",
        "    \n",
        "    # Descompacta o arquivo na pasta de descompactação.\n",
        "    !unzip -o $arquivo -d $DIRETORIO_MODELO\n",
        "\n",
        "    # Baixa o arquivo do vocabulário.\n",
        "    # O vocabulário não está no arquivo compactado acima, mesma url mas arquivo diferente.\n",
        "    URL_MODELO_VOCAB = caminho + arquivo_vocab\n",
        "    !wget $URL_MODELO_VOCAB\n",
        "    \n",
        "    # Coloca o arquivo do vocabulário no diretório de descompactação.\n",
        "    !mv $arquivo_vocab $DIRETORIO_MODELO\n",
        "            \n",
        "    # Move o arquivo para pasta de descompactação\n",
        "    !mv $arquivo $DIRETORIO_MODELO\n",
        "       \n",
        "    print('Pasta do {} pronta!'.format(DIRETORIO_MODELO))\n",
        "\n",
        "    # Lista a pasta corrente.\n",
        "    !ls -la $DIRETORIO_MODELO\n",
        "  else:\n",
        "    print('Variável URL_MODELO não setada!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0AFEvj8wee_"
      },
      "source": [
        "### Download do modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mP2IQKH5wepm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59a5cf98-edbf-4f81-b89e-886133a89633"
      },
      "source": [
        "downloadModelo(model_args.pretrained_model_name_or_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-08 13:28:44--  https://neuralmind-ai.s3.us-east-2.amazonaws.com/nlp/bert-base-portuguese-cased/bert-base-portuguese-cased_pytorch_checkpoint.zip\n",
            "Resolving neuralmind-ai.s3.us-east-2.amazonaws.com (neuralmind-ai.s3.us-east-2.amazonaws.com)... 52.219.102.82\n",
            "Connecting to neuralmind-ai.s3.us-east-2.amazonaws.com (neuralmind-ai.s3.us-east-2.amazonaws.com)|52.219.102.82|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 406220891 (387M) [application/zip]\n",
            "Saving to: ‘bert-base-portuguese-cased_pytorch_checkpoint.zip’\n",
            "\n",
            "bert-base-portugues 100%[===================>] 387.40M  15.8MB/s    in 26s     \n",
            "\n",
            "2021-06-08 13:29:11 (14.8 MB/s) - ‘bert-base-portuguese-cased_pytorch_checkpoint.zip’ saved [406220891/406220891]\n",
            "\n",
            "Archive:  bert-base-portuguese-cased_pytorch_checkpoint.zip\n",
            "  inflating: /content/modelo/config.json  \n",
            "  inflating: /content/modelo/pytorch_model.bin  \n",
            "--2021-06-08 13:29:16--  https://neuralmind-ai.s3.us-east-2.amazonaws.com/nlp/bert-base-portuguese-cased/vocab.txt\n",
            "Resolving neuralmind-ai.s3.us-east-2.amazonaws.com (neuralmind-ai.s3.us-east-2.amazonaws.com)... 52.219.96.24\n",
            "Connecting to neuralmind-ai.s3.us-east-2.amazonaws.com (neuralmind-ai.s3.us-east-2.amazonaws.com)|52.219.96.24|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 209528 (205K) [text/plain]\n",
            "Saving to: ‘vocab.txt’\n",
            "\n",
            "vocab.txt           100%[===================>] 204.62K   357KB/s    in 0.6s    \n",
            "\n",
            "2021-06-08 13:29:18 (357 KB/s) - ‘vocab.txt’ saved [209528/209528]\n",
            "\n",
            "Pasta do /content/modelo pronta!\n",
            "total 824896\n",
            "drwxr-xr-x 2 root root      4096 Jun  8 13:29 .\n",
            "drwxr-xr-x 1 root root      4096 Jun  8 13:29 ..\n",
            "-rw-r--r-- 1 root root 406220891 Jan 22  2020 bert-base-portuguese-cased_pytorch_checkpoint.zip\n",
            "-rw-rw-r-- 1 root root       873 Jan 12  2020 config.json\n",
            "-rw-rw-r-- 1 root root 438235074 Jan 12  2020 pytorch_model.bin\n",
            "-rw-r--r-- 1 root root    209528 Jan 21  2020 vocab.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bcpd9t9PpkrX"
      },
      "source": [
        "## 3.2 Tokenizador(tokenizer) BERT\n",
        "\n",
        "O tokenizador utiliza WordPiece, veja em [artigo original](https://arxiv.org/pdf/1609.08144.pdf).\n",
        "\n",
        "Carregando o tokenizador da pasta '/content/modelo/' do diretório padrão se variável `URL_MODELO` setada.\n",
        "\n",
        "**Caso contrário carrega da comunidade**\n",
        "\n",
        "Por default(`do_lower_case=True`) todas as letras são colocadas para minúsculas. Para ignorar a conversão para minúsculo use o parâmetro `do_lower_case=False`. Esta opção também considera as letras acentuadas(ãçéí...), que são necessárias a língua portuguesa.\n",
        "\n",
        "O parâmetro `do_lower_case` interfere na quantidade tokens a ser gerado apartir de um texto. Quando igual a `False` reduz a quantidade de tokens gerados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeUi177TwUmW"
      },
      "source": [
        "### Função carrega tokenizador"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8cKVs4fpkrY"
      },
      "source": [
        "def carregaTokenizador(MODELO):\n",
        "\n",
        "  # Importando as bibliotecas do tokenizador.\n",
        "  from transformers import BertTokenizer\n",
        "\n",
        "  # Variável para setar o arquivo.\n",
        "  URL_MODELO = None\n",
        "\n",
        "  if 'http' in MODELO:\n",
        "    URL_MODELO = MODELO\n",
        "\n",
        "  # Se a variável URL_MODELO foi setada.\n",
        "  if URL_MODELO:\n",
        "\n",
        "    # Diretório descompactação.\n",
        "    DIRETORIO_MODELO = '/content/modelo'\n",
        "    \n",
        "    # Carregando o Tokenizador.\n",
        "    print('Carregando o tokenizador BERT do diretório {}...'.format(DIRETORIO_MODELO))\n",
        "\n",
        "    tokenizer = BertTokenizer.from_pretrained(DIRETORIO_MODELO, \n",
        "                                              do_lower_case=model_args.do_lower_case)\n",
        "    \n",
        "  else:\n",
        "    # Carregando o Tokenizador da comunidade.\n",
        "    print('Carregando o tokenizador da comunidade...')\n",
        "    \n",
        "    tokenizer = BertTokenizer.from_pretrained(MODELO, \n",
        "                                              do_lower_case=model_args.do_lower_case)\n",
        "\n",
        "  return tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_JOQKI3wNMe"
      },
      "source": [
        "### Carregando o tokenizador"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltWHFPZ_wNjl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac4fd017-d7ec-44f5-fca8-26ea42570293"
      },
      "source": [
        "tokenizer = carregaTokenizador(model_args.pretrained_model_name_or_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Carregando o tokenizador BERT do diretório /content/modelo...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m__On2g1a--K"
      },
      "source": [
        "## 3.3 Modelo(model) BERT\n",
        "\n",
        "Se a variável `URL_MODELO` estiver setada carrega o modelo do diretório `content/modelo`.\n",
        "\n",
        "Caso contrário carrega da comunidade.\n",
        "\n",
        "Carregando o modelo da pasta '/content/modelo/' do diretório padrão.\n",
        "\n",
        "A implementação do huggingface pytorch inclui um conjunto de interfaces projetadas para uma variedade de tarefas de PNL. Embora essas interfaces sejam todas construídas sobre um modelo treinado de BERT, cada uma possui diferentes camadas superiores e tipos de saída projetados para acomodar suas tarefas específicas de PNL.\n",
        "\n",
        "A documentação para estas pode ser encontrada em [aqui](https://huggingface.co/transformers/v2.2.0/model_doc/bert.html).\n",
        "\n",
        "Por default o modelo está em modo avaliação ou seja `model.eval()`.\n",
        "\n",
        "-----------------------\n",
        "\n",
        "Durante a avaliação do modelo, este retorna um número de diferentes objetos com base em como é configurado na chamada do método `from_pretrained`. \n",
        "\n",
        "Quando definimos `output_hidden_states = True` na chamada do método `from_pretrained`, retorno do modelo possui no terceiro item os estados ocultos(**hidden_states**) de todas as camadas.  Veja a documentação para mais detalhes: https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
        "\n",
        "Quando **`output_hidden_states = True`** model retorna:\n",
        "- outputs[0] = last_hidden_state;\n",
        "- outputs[1] = pooler_output; \n",
        "- outputs[2] = hidden_states.\n",
        "\n",
        "Quando **`output_hidden_states = False`** ou não especificado model retorna:\n",
        "- outputs[0] = last_hidden_state;\n",
        "- outputs[1] = pooler_output.\n",
        "\n",
        "\n",
        "**ATENÇÃO**: O parâmetro ´**output_hidden_states = True**´ habilita gerar as camadas ocultas do modelo. Caso contrário somente a última camada é mantida. Este parâmetro otimiza a memória mas não os resultados.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlhhexvMxAQ6"
      },
      "source": [
        "### Função carrega modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRV6l_I-qg9s"
      },
      "source": [
        "def carregaModelo(MODELO):\n",
        "\n",
        "  # Importando as bibliotecas do Modelo\n",
        "  from transformers import BertForSequenceClassification\n",
        "\n",
        "  # Variável para setar o arquivo.\n",
        "  URL_MODELO = None\n",
        "\n",
        "  if 'http' in MODELO:\n",
        "    URL_MODELO = MODELO\n",
        "\n",
        "  # Se a variável URL_MODELO foi setada\n",
        "  if URL_MODELO:\n",
        "\n",
        "    # Diretório descompactação.\n",
        "    DIRETORIO_MODELO = '/content/modelo'\n",
        "    \n",
        "    # Carregando o Modelo BERT\n",
        "    print('Carregando o modelo BERT do diretório {}...'.format(DIRETORIO_MODELO))\n",
        "\n",
        "    model = BertForSequenceClassification.from_pretrained(DIRETORIO_MODELO,   \n",
        "                                                          num_labels = model_args.num_labels,\n",
        "                                                          output_attentions = model_args.output_attentions,\n",
        "                                                          output_hidden_states = model_args.output_hidden_states)\n",
        "  else:\n",
        "    # Carregando o Modelo BERT da comunidade\n",
        "    print('Carregando o modelo BERT da comunidade ...')\n",
        "    \n",
        "    model = BertForSequenceClassification.from_pretrained(MODELO,\n",
        "                                                          num_labels = model_args.num_labels,                                                       \n",
        "                                                          output_attentions = model_args.output_attentions,\n",
        "                                                          output_hidden_states = model_args.output_hidden_states)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnJJngw4xI2X"
      },
      "source": [
        "### Carregando o modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTDWy7pKxNJr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb443bdc-4342-4ada-d8ca-0e0e7cc09cee"
      },
      "source": [
        "model = carregaModelo(model_args.pretrained_model_name_or_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Carregando o modelo BERT do diretório /content/modelo...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at /content/modelo were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/modelo and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJdbTzeejhOE"
      },
      "source": [
        "# 4 Treino"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7MlEvcbuyCh"
      },
      "source": [
        "## 4.1 wandb\n",
        "\n",
        "https://wandb.ai/osmar-braz/ajustefinocstnews_v1_c_sb_holdout/table?workspace=user-osmar-braz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unllEBZQuLkk"
      },
      "source": [
        "### Função de inicialização wandb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0ky2-JguMBG"
      },
      "source": [
        "def inicializacaoWandb():\n",
        "\n",
        "  if model_args.use_wandb:\n",
        "\n",
        "    # Importando a biblioteca.\n",
        "    import wandb\n",
        "\n",
        "    # Inicializando o registro do experimento.\n",
        "    # Na execução só pode existir de um init  para que não gere dois registros no wandb.\n",
        "    wandb.init(project=\"ajustefinocstnews_avaliacaomoodle_v1_c_sb\", name=training_args.output_dir)\n",
        "\n",
        "    # Atualiza os parâmetros de treinamento no wandb.\n",
        "    wandb.config.update(training_args)\n",
        "    # Atualiza os parâmetros do modelo no wandb.\n",
        "    wandb.config.update(model_args)\n",
        "\n",
        "    # Registra os parämetros não literais do model_args.\n",
        "    wandb.log({\"max_seq_len\": model_args.max_seq_len})\n",
        "    wandb.log({\"do_lower_case\": model_args.do_lower_case})\n",
        "    wandb.log({\"output_hidden_states\": model_args.output_hidden_states})\n",
        "\n",
        "    return wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrKMXRNm7OI6"
      },
      "source": [
        "### Inicialização wandb\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UDfEld07QRm"
      },
      "source": [
        "wandb = inicializacaoWandb()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJzDVmgts4nd"
      },
      "source": [
        "## 4.2 Colab GPU\n",
        "\n",
        "Conecta o modelo carregado do BERT a GPU para reduzir o tempo de processamento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YRD8msZyLWh"
      },
      "source": [
        "### Função conecta GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3x2zp5UnCN4E"
      },
      "source": [
        "def conectaGPU(model):\n",
        "  \n",
        "  # Associa a GPU ao modelo.\n",
        "  model.to(device)\n",
        "\n",
        "  # Se existe GPU disponível.\n",
        "  if torch.cuda.is_available():    \n",
        "    # Diga ao pytorch para rodar este modelo na GPU.\n",
        "    print(\"Pytorch rodando o modelo na GPU\")\n",
        "    model.cuda()\n",
        "  else:\n",
        "    print(\"Pytorch rodando sem GPU\")\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlNQLdiNySKQ"
      },
      "source": [
        "### Conectando GPU ao modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46lfiIZfySZd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d74a062c-4bbd-4a15-995b-674e825feedf"
      },
      "source": [
        "model = conectaGPU(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pytorch rodando o modelo na GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-aD0Bl2CN4E"
      },
      "source": [
        "## 4.3 Arquivo dos dados de treino"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBQ2aL5ny86T"
      },
      "source": [
        "### função de download dos arquivos de dados\n",
        "\n",
        "Download do arquivo dos dados de uma pasta pública no meu OneDrive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2ZZ0SH9K-bi"
      },
      "source": [
        "def downloadArquivoDados():\n",
        "  \n",
        "  # Nome do arquivo a ser criado.\n",
        "  NOME_ARQUIVO = \"Summarycoherencemodels.zip\"\n",
        "\n",
        "  # Apaga o arquivo.\n",
        "  !rm '$NOME_ARQUIVO'\n",
        "\n",
        "  # Realiza o download do arquivo da url especificada\n",
        "  !wget https://sites.icmc.usp.br/taspardo/Summary%20coherence%20models.zip\n",
        "\n",
        "  # Lista o diretório corrente e os arquivos.\n",
        "  !pwd\n",
        "  !ls -la\n",
        "\n",
        "  # Descompactando os arquivos\n",
        "  # Lista o diretório corrente e os arquivos.\n",
        "  !pwd\n",
        "  !ls -la\n",
        "\n",
        "  # Apaga o diretório e seus arquivos\n",
        "  !rm -rf \"Summary coherence model\"\n",
        "\n",
        "  # Descompacta o arquivo\n",
        "  !unzip -o '$NOME_ARQUIVO'\n",
        "\n",
        "  # Lista os arquivos do diretório corrente\n",
        "  !ls -la\n",
        "\n",
        "  # Descompactando os experimentos\n",
        "  NOME_ARQUIVO_EXPERIMENTO = 'Modelo Latent Semantic Analysis.zip'\n",
        "\n",
        "  # Lista o diretório corrente e os arquivos.\n",
        "  !pwd\n",
        "  !ls -la\n",
        "\n",
        "  # Apaga o diretório 'Modelo Latent Semantic Analysis' e seus arquivos\n",
        "  !rm -rf 'Modelo Latent Semantic Analysis'\n",
        "\n",
        "  # Descompacta o arquivo o experimento\n",
        "  !unzip -o '$NOME_ARQUIVO_EXPERIMENTO'\n",
        "\n",
        "  # Lista os arquivos do diretório corrente\n",
        "  !ls -la"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUAOltxAzwtj"
      },
      "source": [
        "### Executando o download do arquivo de dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EykAme_wzw5l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ef25d3a-181e-4671-9211-a99b830340af"
      },
      "source": [
        "downloadArquivoDados()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'Summarycoherencemodels.zip': No such file or directory\n",
            "--2021-07-12 19:53:12--  https://sites.icmc.usp.br/taspardo/Summary%20coherence%20models.zip\n",
            "Resolving sites.icmc.usp.br (sites.icmc.usp.br)... 143.107.183.230\n",
            "Connecting to sites.icmc.usp.br (sites.icmc.usp.br)|143.107.183.230|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 498024248 (475M) [application/zip]\n",
            "Saving to: ‘Summary coherence models.zip’\n",
            "\n",
            "s.zip                 7%[>                   ]  34.54M   144KB/s    eta 82m 44s^C\n",
            "/content\n",
            "total 35424\n",
            "drwxr-xr-x 1 root root     4096 Jul 12 19:53  .\n",
            "drwxr-xr-x 1 root root     4096 Jul 12 19:46  ..\n",
            "drwxr-xr-x 4 root root     4096 Jul  1 13:41  .config\n",
            "drwxr-xr-x 1 root root     4096 Jul  1 13:42  sample_data\n",
            "-rw-r--r-- 1 root root 36257792 Jul 12 19:59 'Summary coherence models.zip'\n",
            "/content\n",
            "total 35424\n",
            "drwxr-xr-x 1 root root     4096 Jul 12 19:53  .\n",
            "drwxr-xr-x 1 root root     4096 Jul 12 19:46  ..\n",
            "drwxr-xr-x 4 root root     4096 Jul  1 13:41  .config\n",
            "drwxr-xr-x 1 root root     4096 Jul  1 13:42  sample_data\n",
            "-rw-r--r-- 1 root root 36257792 Jul 12 19:59 'Summary coherence models.zip'\n",
            "unzip:  cannot find or open Summarycoherencemodels.zip, Summarycoherencemodels.zip.zip or Summarycoherencemodels.zip.ZIP.\n",
            "total 35424\n",
            "drwxr-xr-x 1 root root     4096 Jul 12 19:53  .\n",
            "drwxr-xr-x 1 root root     4096 Jul 12 19:46  ..\n",
            "drwxr-xr-x 4 root root     4096 Jul  1 13:41  .config\n",
            "drwxr-xr-x 1 root root     4096 Jul  1 13:42  sample_data\n",
            "-rw-r--r-- 1 root root 36257792 Jul 12 19:59 'Summary coherence models.zip'\n",
            "/content\n",
            "total 35424\n",
            "drwxr-xr-x 1 root root     4096 Jul 12 19:53  .\n",
            "drwxr-xr-x 1 root root     4096 Jul 12 19:46  ..\n",
            "drwxr-xr-x 4 root root     4096 Jul  1 13:41  .config\n",
            "drwxr-xr-x 1 root root     4096 Jul  1 13:42  sample_data\n",
            "-rw-r--r-- 1 root root 36257792 Jul 12 19:59 'Summary coherence models.zip'\n",
            "unzip:  cannot find or open Modelo Latent Semantic Analysis.zip, Modelo Latent Semantic Analysis.zip.zip or Modelo Latent Semantic Analysis.zip.ZIP.\n",
            "total 35424\n",
            "drwxr-xr-x 1 root root     4096 Jul 12 19:53  .\n",
            "drwxr-xr-x 1 root root     4096 Jul 12 19:46  ..\n",
            "drwxr-xr-x 4 root root     4096 Jul  1 13:41  .config\n",
            "drwxr-xr-x 1 root root     4096 Jul  1 13:42  sample_data\n",
            "-rw-r--r-- 1 root root 36257792 Jul 12 19:59 'Summary coherence models.zip'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdkkxzscF4Yi"
      },
      "source": [
        "## 4.4 Preparação dos dados Treino"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiGs9RXZ4a5c"
      },
      "source": [
        "### Função carregamento dados\n",
        "\n",
        "Carrega os dados dos arquivos e uma lista e converte em um dataframe\n",
        "\n",
        "Atributos do dataframe:\n",
        "0. \"idOriginal\" - Nome do arquivo original\n",
        "1. \"sentencasOriginais\" - Lista das sentenças do documento original\n",
        "2. \"documentoOriginal\" - Documento original\n",
        "3. \"idPermutado\" - Nome do arquivo permutado\n",
        "4. \"sentencasPermutadas\" - Lista das sentenças do documento permtuado\n",
        "5. \"documentoPermutado\" - Documento permutado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3pHV7i14dlt"
      },
      "source": [
        "def carregamentoDados():\n",
        "\n",
        "  # Import das bibliotecas\n",
        "  import os\n",
        "\n",
        "  ############################################################\n",
        "  # ORIGINAIS\n",
        "  ############################################################\n",
        "\n",
        "  lista_documentos_originais = []\n",
        "\n",
        "  arquivos = os.listdir('/content/Modelo Latent Semantic Analysis/Sumarios_Humanos/') #Entrada (Input) - diretório de sumários humanos e permutados\n",
        "\n",
        "  if '.DS_Store' in arquivos:\n",
        "    arquivos.remove('.DS_Store')\n",
        "\n",
        "  for i in range(len(arquivos)):\n",
        "    # Recupera a posição do ponto no nome do arquivo\n",
        "    ponto = arquivos[i].find('.')\n",
        "    # Recupera o nome do arquivo até a posição do ponto\n",
        "    nomeArquivo = arquivos[i][:ponto]\n",
        "\n",
        "    documento = carregar('/content/Modelo Latent Semantic Analysis/Sumarios_Humanos/'+arquivos[i])\n",
        "\n",
        "    lista_documentos_originais.append([documento,1])\n",
        "    \n",
        "  print ('TERMINADO ORIGINAIS')\n",
        "\n",
        "  # Import das bibliotecas\n",
        "  import os\n",
        "\n",
        "  ############################################################\n",
        "  # PERMUTADOS\n",
        "  ############################################################\n",
        "\n",
        "  lista_documentos_permutados = []\n",
        "\n",
        "  arquivos = os.listdir('/content/Modelo Latent Semantic Analysis/Sumarios_Humanos_Permutados/') #Entrada (Input) - diret�rio de sum�rios humanos e permutados\n",
        "\n",
        "  if '.DS_Store' in arquivos:\n",
        "    arquivos.remove('.DS_Store')\n",
        "\n",
        "  for i in range(len(arquivos)):\n",
        "    # Recupera a posição do ponto no nome do arquivo\n",
        "    ponto = arquivos[i].find('.')\n",
        "    # Recupera o nome do arquivo até a posição do ponto\n",
        "    nomeArquivo = arquivos[i][:ponto]\n",
        "\n",
        "    documento = carregar('/content/Modelo Latent Semantic Analysis/Sumarios_Humanos_Permutados/'+arquivos[i])\n",
        "\n",
        "    lista_documentos_permutados.append([documento,0])\n",
        "    \n",
        "  print ('TERMINADO INCOERENTES')\n",
        "\n",
        "  print(len(lista_documentos_originais))\n",
        "  print(len(lista_documentos_permutados))\n",
        "\n",
        "  # Gerando os pares de documentos originais e permutados\n",
        "\n",
        "  # Lista dos documentos originais e permutados \n",
        "  lista_documentos = []\n",
        "\n",
        "  arquivosOriginais = os.listdir('/content/Modelo Latent Semantic Analysis/Sumarios_Humanos/') #Entrada (Input) - diretório de sumários humanos e permutados\n",
        "\n",
        "  #del x[0](Comentando, pois o arquivo \".DS_Store\" não está no início da lista!).\n",
        "  if '.DS_Store' in arquivosOriginais:\n",
        "    arquivosOriginais.remove('.DS_Store')\n",
        "\n",
        "  for i in range(len(arquivosOriginais)):\n",
        "\n",
        "    # Recupera a posição do ponto no nome do arquivo.\n",
        "    ponto = arquivosOriginais[i].find('.')\n",
        "    # Recupera o nome do arquivo até a posição do ponto.\n",
        "    arquivoOriginal = arquivosOriginais[i][:ponto]\n",
        "\n",
        "    # Carrega o documento original.\n",
        "    # Carrega como parágrafo\n",
        "    documentoOriginal = carregar('/content/Modelo Latent Semantic Analysis/Sumarios_Humanos/'+arquivosOriginais[i])\n",
        "    # Carrega uma lista das sentenças\n",
        "    sentencasOriginais = carregarLista('/content/Modelo Latent Semantic Analysis/Sumarios_Humanos/'+arquivosOriginais[i])\n",
        "\n",
        "    # Percorre as 20 permutações.\n",
        "    for j in range(20):\n",
        "        # Recupera o nome do arquivo permutado.\n",
        "        arquivoPermutado = arquivoOriginal + '_Perm_'+str(j)+'.txt'\n",
        "\n",
        "        # Carrega o arquivo permutado.\n",
        "        documentoPermutado = carregar('/content/Modelo Latent Semantic Analysis/Sumarios_Humanos_Permutados/'+ arquivoPermutado)\n",
        "        sentencasPermutadas = carregarLista('/content/Modelo Latent Semantic Analysis/Sumarios_Humanos_Permutados/'+ arquivoPermutado)\n",
        "\n",
        "        # Adiciona o par original e sua versão permutada.\n",
        "        lista_documentos.append([arquivosOriginais[i], sentencasOriginais, documentoOriginal, arquivoPermutado, sentencasPermutadas, documentoPermutado])\n",
        "\n",
        "  print(len(lista_documentos))\n",
        "\n",
        "  # Converte a lista em um dataframe\n",
        "\n",
        "  # Import das bibliotecas.\n",
        "  import pandas as pd\n",
        "\n",
        "  # Converte a lista em um dataframe.\n",
        "  dfdados = pd.DataFrame.from_records(lista_documentos, columns=['idOriginal','sentencasOriginais','documentoOriginal','idPermutado','sentencasPermutadas','documentoPermutado'])\n",
        "\n",
        "  # Número de linhas carregadas do arquivo.\n",
        "  print('Total de registros              : {}'.format(len(dfdados)))\n",
        "\n",
        "  # Organiza os dados\n",
        "\n",
        "  dados_organizados = []\n",
        "\n",
        "  # Coloca o par um embaixo do outro.\n",
        "  for index, linha in dfdados.iterrows():        \n",
        "    # 1 Para original\n",
        "    dados_organizados.append((linha['idOriginal'],linha['documentoOriginal'],1))    \n",
        "    # 0 para uma permutação \n",
        "    dados_organizados.append((linha['idPermutado'],linha['documentoPermutado'],0))\n",
        "\n",
        "  # Cria um dataframe com os dados\n",
        "  dfdados = pd.DataFrame(dados_organizados, columns=[\"id\",\"documento\",\"classe\"])      \n",
        "\n",
        "  return dfdados \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snzt2Fd85h6t"
      },
      "source": [
        "### Carregamento dos dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lI1CcD9z5kKk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3e8baf7-ba05-41c4-cb0d-8a1e1d746215"
      },
      "source": [
        "dfdados = carregamentoDados()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TERMINADO ORIGINAIS\n",
            "TERMINADO INCOERENTES\n",
            "251\n",
            "5020\n",
            "5020\n",
            "Total de registros              : 5020\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PEU6dof5qUq"
      },
      "source": [
        "### Função descarte documentos muito grandes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdC5kOLg5t59"
      },
      "source": [
        "def descarteDocumentosGrandes(tamanho_maximo_token, dfdados):\n",
        "  \n",
        "  # Define o tamanho máximo para os tokens.\n",
        "  tamanho_maximo = tamanho_maximo_token\n",
        "\n",
        "  # Tokenize a codifica as setenças para o BERT.     \n",
        "  dfdados['input_ids'] = dfdados['documento'].apply(lambda tokens: tokenizer.encode(tokens, add_special_tokens=True))\n",
        "        \n",
        "  dfdados = dfdados[dfdados['input_ids'].apply(len)<tamanho_maximo]\n",
        "\n",
        "  print('Tamanho do conjunto de dados: {}'.format(len(dfdados)))\n",
        "\n",
        "  # Remove as colunas desnecessárias.\n",
        "  dfdados = dfdados.drop(columns=['input_ids'])\n",
        "\n",
        "  # Informações do DataFrame\n",
        "  print(dfdados.info())\n",
        "\n",
        "  return dfdados"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6InuUN12wri"
      },
      "source": [
        "### Descartando documentos muito grandes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOE0cklnrqgC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00583f76-0f14-4ca3-ad60-fb8cc93312f9"
      },
      "source": [
        "dfdados = descarteDocumentosGrandes(model_args.max_seq_len, dfdados)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-06-08 13:30:31,594 : INFO : NumExpr defaulting to 2 threads.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Tamanho do conjunto de dados: 9960\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 9960 entries, 0 to 10039\n",
            "Data columns (total 3 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   id         9960 non-null   object\n",
            " 1   documento  9960 non-null   object\n",
            " 2   classe     9960 non-null   int64 \n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 311.2+ KB\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYikskWXHuGL"
      },
      "source": [
        "### Divisão do conjunto de dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQpcvM7DHuGT"
      },
      "source": [
        "Divide nosso conjunto de treinamento para usar 70% para treinamento e 30% para validação."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fxqu-C4T6Izp"
      },
      "source": [
        "### Função divisão conjunto de dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ghqvczsy6LTj"
      },
      "source": [
        "def divisaoConjuntoDados(dfdados):\n",
        "  \n",
        "  # Import das bibliotecas.\n",
        "  from sklearn.model_selection import train_test_split\n",
        "\n",
        "  #30% de teste\n",
        "  test_qtde = int(0.3*dfdados.shape[0])\n",
        "  dfdados_train, dfdados_test = train_test_split(dfdados, test_size=test_qtde, random_state=42, stratify=dfdados['classe'])\n",
        "\n",
        "  len(dfdados_train), len(dfdados_test)\n",
        "\n",
        "  return dfdados_train, dfdados_test\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tntNQtk6dwx"
      },
      "source": [
        "### Divisão do conjunto de dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8Rq2VmQHuGU"
      },
      "source": [
        " #dfdados_train, dfdados_test = divisaoConjuntoDados(dfdados)\n",
        " dfdados_train = dfdados"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iotmxm9qHuGV"
      },
      "source": [
        "Vamos extrair os dados do arquivo do TensorFlow, para termos apenas tipos simples de Python.\n",
        "\n",
        "Não foi usada a classe tensorflow_datasets, portanto não foi necessária a extração, somente a divisão em listas separadas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRFutyKu6pNc"
      },
      "source": [
        "### Seleciona as colunas de treino"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfnQJDY_6qey",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab7850c4-45e3-4136-9622-ef41a9d0663f"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "import numpy as np\n",
        "\n",
        "# Pega as listas de documentos e seus rótulos para o treino\n",
        "documentos_treino = dfdados_train.documento.values\n",
        "classes_treino = dfdados_train.classe.values\n",
        "documentoids_treino = dfdados_train.id.values\n",
        "\n",
        "# Mostra algumas estatísticas.\n",
        "print('{:,} Amostras de Treino'.format(len(documentos_treino)))\n",
        "print('{:,} Rótulos de Treino'.format(len(classes_treino)))\n",
        "print('Rótulos: {}'.format(np.unique(classes_treino)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9,960 Amostras de Treino\n",
            "9,960 Rótulos de Treino\n",
            "Rótulos: [0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQUy9Tat2EF_"
      },
      "source": [
        "## 4.5 Análise treino"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX54hJPRHFr7"
      },
      "source": [
        "Usaremos os pandas para analisar o conjunto de dados e examinar algumas de suas propriedades e pontos de dados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08pO03Ff1BjI"
      },
      "source": [
        "Atributos da lista:\n",
        "0. \"arquivo\"\n",
        "1. \"documento\"\n",
        "2. \"classe\" (1-Original, 0-Permutado)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2WkBWvtHFr7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "e9651ba8-c2ca-490f-a1c9-16e972921b2e"
      },
      "source": [
        "dfdados_train.sample(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>documento</th>\n",
              "      <th>classe</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9115</th>\n",
              "      <td>C41_Extrato_3_Perm_17.txt</td>\n",
              "      <td>Thiago Pereira foi mais uma vez a estrela da n...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9585</th>\n",
              "      <td>C3_Extrato_4_Perm_12.txt</td>\n",
              "      <td>Para a TAM, a falha não impediria a realização...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>C49_Extrato_4.txt</td>\n",
              "      <td>O presidente Luiz Inácio Lula da Silva classif...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10039</th>\n",
              "      <td>C46_Extrato_5_Perm_19.txt</td>\n",
              "      <td>Cerca de 1.700 pessoas fugiram de suas casas p...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3023</th>\n",
              "      <td>C36_Extrato_1_Perm_11.txt</td>\n",
              "      <td>Sobreviver politicamente era queestão de homra...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                              id  ... classe\n",
              "9115   C41_Extrato_3_Perm_17.txt  ...      0\n",
              "9585    C3_Extrato_4_Perm_12.txt  ...      0\n",
              "86             C49_Extrato_4.txt  ...      1\n",
              "10039  C46_Extrato_5_Perm_19.txt  ...      0\n",
              "3023   C36_Extrato_1_Perm_11.txt  ...      0\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puijlGwiHFr9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb373de5-952f-45b3-bed1-9d95a8e6bad9"
      },
      "source": [
        "# Mostra o número de documento de treino.\n",
        "print('Número de documentos de treino: {:,}\\n'.format(dfdados_train.shape[0]))\n",
        "\n",
        "# Informações do DataFrame.\n",
        "print(dfdados_train.info())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Número de documentos de treino: 9,960\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 9960 entries, 0 to 10039\n",
            "Data columns (total 3 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   id         9960 non-null   object\n",
            " 1   documento  9960 non-null   object\n",
            " 2   classe     9960 non-null   int64 \n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 311.2+ KB\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdJhSquUzo68"
      },
      "source": [
        "### Distribuição das classes\n",
        "\n",
        "O dataset está bem balanceado, o que nos conduz a utilizar acurácia como métrica."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AV7n7pKGzfOG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "5d2e8c36-83f3-4ba4-8f06-2989578026c3"
      },
      "source": [
        "dfdados_train.groupby('classe').count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>documento</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>classe</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4980</td>\n",
              "      <td>4980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4980</td>\n",
              "      <td>4980</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id  documento\n",
              "classe                 \n",
              "0       4980       4980\n",
              "1       4980       4980"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FYof6pYz9ja",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3daba7fb-5550-4c02-ab0f-dcccac864fc9"
      },
      "source": [
        "# Informações do DataFrame.\n",
        "print(dfdados_train.info())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 9960 entries, 0 to 10039\n",
            "Data columns (total 3 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   id         9960 non-null   object\n",
            " 1   documento  9960 non-null   object\n",
            " 2   classe     9960 non-null   int64 \n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 311.2+ KB\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRp4O7D295d_"
      },
      "source": [
        "### Conjunto de dados em Treinamento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AeCq7cbo5Mu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db076412-ae76-48f5-da83-244608350972"
      },
      "source": [
        "# Mostra o resultado dos dados carregados.\n",
        "print(\"Total do conjunto de dados          : {}.\".format(len(dfdados)))\n",
        "print(\"Total do conjunto de dados de treino: {}.\".format(len(documentos_treino)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total do conjunto de dados          : 9960.\n",
            "Total do conjunto de dados de treino: 9960.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--c3qOsWSSfQ"
      },
      "source": [
        "## 4.6 Arquivo dos dados avaliacao"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKYdoxlxSSfY"
      },
      "source": [
        "### Função de download dos arquivos de dados\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lpH27BySSfY"
      },
      "source": [
        "def downloadArquivoDados():\n",
        "  \n",
        "  # Especifica o nome do arquivo de dados\n",
        "  # Nome do arquivo\n",
        "  NOMEARQUIVOORIGINAL = 'original.zip'\n",
        "  NOMEARQUIVOPERMUTADO = 'permutado.zip'\n",
        "\n",
        "  # Especifica o caminho dos arquivos\n",
        "  # Define o caminho e nome do arquivo de dados\n",
        "  CAMINHOARQUIVOORIGINAL = '/content/drive/MyDrive/Colab Notebooks/Data/Moodle/dadosmoodle_documento_pergunta_sentenca_intervalo/' + NOMEARQUIVOORIGINAL\n",
        "  CAMINHOARQUIVOPERMUTADO = '/content/drive/MyDrive/Colab Notebooks/Data/Moodle/dadosmoodle_documento_pergunta_sentenca_intervalo/' + NOMEARQUIVOPERMUTADO\n",
        "  \n",
        "  # Copia os arquivos do Google Drive para o Colaboratory\n",
        "  # Copia os arquivos dos dados originais e permutados par ao diretório corrente.\n",
        "\n",
        "  !cp '/content/drive/MyDrive/Colab Notebooks/Data/Moodle/dadosmoodle_documento_pergunta_sentenca_intervalo/'$NOMEARQUIVOORIGINAL .\n",
        "  !cp '/content/drive/MyDrive/Colab Notebooks/Data/Moodle/dadosmoodle_documento_pergunta_sentenca_intervalo/'$NOMEARQUIVOPERMUTADO .\n",
        "\n",
        "  print(\"Terminei a cópia!\")\n",
        "\n",
        "  # Descompacta os arquivos\n",
        "  !unzip -o -q $NOMEARQUIVOORIGINAL\n",
        "  !unzip -o -q $NOMEARQUIVOPERMUTADO\n",
        "\n",
        "  print(\"Terminei a descompactação!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDPTkeIHOF7s"
      },
      "source": [
        "### Executando o download do arquivo de dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHDs7XahOGI7",
        "outputId": "c6a754c2-1550-49f2-b16e-b9c0d633af7f"
      },
      "source": [
        "downloadArquivoDados()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Terminei a cópia!\n",
            "Terminei a descompactação!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwNEKEHHZUk5"
      },
      "source": [
        "## 4.7 Preparação dos dados avaliação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNg8mirzSSfb"
      },
      "source": [
        "### Função carregamento dados\n",
        "\n",
        "Carrega os dados dos arquivos e uma lista e converte em um dataframe\n",
        "\n",
        "Atributos do dataframe:\n",
        "0. \"idOriginal\" - Nome do arquivo original\n",
        "1. \"sentencasOriginais\" - Lista das sentenças do documento original\n",
        "2. \"documentoOriginal\" - Documento original\n",
        "3. \"idPermutado\" - Nome do arquivo permutado\n",
        "4. \"sentencasPermutadas\" - Lista das sentenças do documento permtuado\n",
        "5. \"documentoPermutado\" - Documento permutado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmtEAzwJSSfb"
      },
      "source": [
        "def carregamentoDados():\n",
        "\n",
        "  # Biblioteca para acessar o sistema de arquivos\n",
        "  import os\n",
        "\n",
        "  ############################################################\n",
        "  # Original\n",
        "  ############################################################\n",
        "\n",
        "  lista_documentos_originais = []\n",
        "\n",
        "  arquivos = os.listdir('/content/dadosmoodle_documento_pergunta_sentenca_intervalo/original/') #Entrada (Input)\n",
        "\n",
        "  # Percorre a lista de arquivos do diretório\n",
        "  for i in range(len(arquivos)):\n",
        "    # Recupera a posição do ponto no nome do arquivo\n",
        "    ponto = arquivos[i].find('.')\n",
        "    # Recupera o nome do arquivo até a posição do ponto\n",
        "    nomeArquivo = arquivos[i][:ponto]\n",
        "\n",
        "    # Carrega o arquivo de nome x[i] do diretório\n",
        "    documento = carregarLista('/content/dadosmoodle_documento_pergunta_sentenca_intervalo/original/'+arquivos[i])\n",
        "\n",
        "    lista_documentos_originais.append([nomeArquivo,documento])\n",
        "    \n",
        "  print ('TERMINADO ORIGINAL')\n",
        "  print(len(lista_documentos_originais))\n",
        "\n",
        "  # Biblioteca para acessar o sistema de arquivos\n",
        "  import os\n",
        "\n",
        "  ############################################################\n",
        "  # Permutado\n",
        "  ############################################################\n",
        "\n",
        "  lista_documentos_permutados = []\n",
        "\n",
        "  arquivos = os.listdir('/content/dadosmoodle_documento_pergunta_sentenca_intervalo/permutado/') #Entrada (Input)\n",
        "\n",
        "  # Percorre a lista de arquivos do diretório\n",
        "  for i in range(len(arquivos)):\n",
        "    # Recupera a posição do ponto no nome do arquivo\n",
        "    ponto = arquivos[i].find('.')\n",
        "    # Recupera o nome do arquivo até a posição do ponto\n",
        "    nomeArquivo = arquivos[i][:ponto]\n",
        "\n",
        "    # Carrega o arquivo de nome x[i] do diretório\n",
        "    documento = carregarLista('/content/dadosmoodle_documento_pergunta_sentenca_intervalo/permutado/'+arquivos[i])\n",
        "\n",
        "    # Adiciona a lista o conteúdo do arquivo\n",
        "    lista_documentos_permutados.append([nomeArquivo,documento])\n",
        "    \n",
        "  print ('TERMINADO ORIGINAL')\n",
        "  print(len(lista_documentos_permutados))\n",
        "\n",
        "  # Gerando os pares de documentos originais e permutados\n",
        "\n",
        "  # Lista dos documentos originais e permutados unificados\n",
        "  lista_documentos = []\n",
        "\n",
        "  # Percorre a lista dos documentos originais\n",
        "  for i,linha_original in enumerate(lista_documentos_originais):\n",
        "\n",
        "    # Percorre a lista dos documentos permutados\n",
        "    for j,linha_permutado in enumerate(lista_documentos_permutados):\n",
        "\n",
        "        # Se o nome o id documento original está no nome do documento permutado \n",
        "        if linha_original[0] in linha_permutado[0]:\n",
        "\n",
        "          # Adiciona os dados a lista dos documentos\n",
        "          documentoOriginal = \" \".join(linha_original[1])\n",
        "          documentoPermutado = \" \".join(linha_permutado[1])\n",
        "\n",
        "          # Adiciona os dados originais e permutados a lista\n",
        "          lista_documentos.append([linha_original[0],linha_original[1], documentoOriginal, linha_permutado[0],linha_permutado[1], documentoPermutado ])\n",
        "\n",
        "  print(len(lista_documentos))\n",
        "\n",
        "  # Converte a lista em um dataframe\n",
        "\n",
        "  # Import das bibliotecas.\n",
        "  import pandas as pd\n",
        "\n",
        "  # Converte a lista em um dataframe.\n",
        "  dfdados = pd.DataFrame.from_records(lista_documentos, columns=['idOriginal','sentencasOriginais','documentoOriginal','idPermutado','sentencasPermutadas','documentoPermutado'])\n",
        "\n",
        "  # Número de linhas carregadas do arquivo.\n",
        "  print('Total de registros              : {}'.format(len(dfdados)))\n",
        "\n",
        "  # Organiza os dados\n",
        "  \n",
        "  dados_organizados = []\n",
        "\n",
        "  # Coloca o par um embaixo do outro.\n",
        "  for index, linha in dfdados.iterrows():        \n",
        "    # 1 Para original\n",
        "    dados_organizados.append((linha['idOriginal'],linha['documentoOriginal'],1))    \n",
        "    # 0 para uma permutação \n",
        "    dados_organizados.append((linha['idPermutado'],linha['documentoPermutado'],0))\n",
        "\n",
        "  # Cria um dataframe com os dados de teste\n",
        "  dfdados = pd.DataFrame(dados_organizados, columns=[\"id\",\"documento\",\"classe\"])      \n",
        "\n",
        "  return dfdados "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djlNBt70SSfb"
      },
      "source": [
        "### Carregamento dos dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iqWwtn0SSfb",
        "outputId": "5f805538-f0d2-41ad-ad3e-dde13b3c4df8"
      },
      "source": [
        "dfdados = carregamentoDados()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TERMINADO ORIGINAL\n",
            "561\n",
            "TERMINADO ORIGINAL\n",
            "11220\n",
            "11220\n",
            "Total de registros              : 11220\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYIjeR2lPSLL"
      },
      "source": [
        "### Descartando os documentos muito grandes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YD2QP1qSSfc"
      },
      "source": [
        "### Função descarte documentos muito grandes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0b1TwonSSfd"
      },
      "source": [
        "def descarteDocumentosGrandes(tamanho_maximo_token, dfdados):\n",
        "  \n",
        "  # Define o tamanho máximo para os tokens.\n",
        "  tamanho_maximo = tamanho_maximo_token\n",
        "\n",
        "  # Tokenize a codifica as setenças para o BERT.     \n",
        "  dfdados['input_ids'] = dfdados['documento'].apply(lambda tokens: tokenizer.encode(tokens, add_special_tokens=True))\n",
        "        \n",
        "  dfdados = dfdados[dfdados['input_ids'].apply(len)<tamanho_maximo]\n",
        "\n",
        "  print('Tamanho do conjunto de dados: {}'.format(len(dfdados)))\n",
        "\n",
        "  # Remove as colunas desnecessárias.\n",
        "  dfdados = dfdados.drop(columns=['input_ids'])\n",
        "\n",
        "  # Informações do DataFrame\n",
        "  print(dfdados.info())\n",
        "\n",
        "  return dfdados"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmMnVmwcwj5G"
      },
      "source": [
        "### Descartando os documentos muito grandes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diKKgDuuwmwY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03697e65-25dc-48b5-d03f-c55ed1a16117"
      },
      "source": [
        "dfdados = descarteDocumentosGrandes(model_args.max_seq_len, dfdados)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tamanho do conjunto de dados: 22440\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 22440 entries, 0 to 22439\n",
            "Data columns (total 3 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   id         22440 non-null  object\n",
            " 1   documento  22440 non-null  object\n",
            " 2   classe     22440 non-null  int64 \n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 701.2+ KB\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPLN01DxPmO-"
      },
      "source": [
        "### Divisão do conjunto de dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPTPI6WjPmO-"
      },
      "source": [
        "Divide nosso conjunto de treinamento para usar 70% para treinamento e 30% para validação."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rgk6-Ct7SSfd"
      },
      "source": [
        "### Função divisão conjunto de dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nURrXOaSSfe"
      },
      "source": [
        "def divisaoConjuntoDados(dfdados):\n",
        "  \n",
        "  # Import das bibliotecas.\n",
        "  from sklearn.model_selection import train_test_split\n",
        "\n",
        "  #30% de teste\n",
        "  test_qtde = int(0.3*dfdados.shape[0])\n",
        "  dfdados_train, dfdados_test = train_test_split(dfdados, test_size=test_qtde, random_state=42, stratify=dfdados['classe'])\n",
        "\n",
        "  len(dfdados_train), len(dfdados_test)\n",
        "\n",
        "  return dfdados_train, dfdados_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnQfhbo-SSfe"
      },
      "source": [
        "### Divisão do conjunto de dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZwcfmfIPs1t"
      },
      "source": [
        " #dfdados_train, dfdados_test = divisaoConjuntoDados(dfdados)\n",
        " dfdados_test = dfdados"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMLCdOMRSSfe"
      },
      "source": [
        "Vamos extrair os dados do arquivo do TensorFlow, para termos apenas tipos simples de Python.\n",
        "\n",
        "Não foi usada a classe tensorflow_datasets, portanto não foi necessária a extração, somente a divisão em listas separadas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHcN8zf4QHUG"
      },
      "source": [
        "### Seleciona as colunas de teste"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPWgwot2HuGW",
        "outputId": "ca9c0f7f-434c-4700-877b-f20f6156549c"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "import numpy as np\n",
        "\n",
        "# Pega as listas de documentos e seus rótulos para o treino\n",
        "documentos_teste = dfdados_test.documento.values\n",
        "classes_teste = dfdados_test.classe.values\n",
        "documentoids_teste = dfdados_test.id.values\n",
        "\n",
        "# Mostra algumas estatísticas.\n",
        "print('{:,} Amostras de Teste'.format(len(documentos_teste)))\n",
        "print('{:,} Rótulos de Teste'.format(len(classes_teste)))\n",
        "print('Rótulos: {}'.format(np.unique(classes_teste)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22,440 Amostras de Teste\n",
            "22,440 Rótulos de Teste\n",
            "Rótulos: [0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCXgM-sxSSff"
      },
      "source": [
        "## 4.8 Análise avaliação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwfRyHpDSSff"
      },
      "source": [
        "Usaremos os pandas para analisar o conjunto de dados e examinar algumas de suas propriedades e pontos de dados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16LsLxcOSSff"
      },
      "source": [
        "Atributos da lista:\n",
        "0. \"arquivo\"\n",
        "1. \"documento\"\n",
        "2. \"classe\" (1-Original, 0-Permutado)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q10hRk3XICB7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "b427e6fd-0729-49b7-85fc-651035cb372d"
      },
      "source": [
        "dfdados_test.sample(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>documento</th>\n",
              "      <th>classe</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>14157</th>\n",
              "      <td>Documento_74940_Perm_1</td>\n",
              "      <td>Ficou alguma dúvida quanto ao exercício e à pe...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5362</th>\n",
              "      <td>Documento_47613</td>\n",
              "      <td>A tirinha acima nos mostra, a que a TV, como m...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10085</th>\n",
              "      <td>Documento_103572_Perm_6</td>\n",
              "      <td>Olá Simone tudo bem? No cadastro de cliente fa...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21301</th>\n",
              "      <td>Documento_68115_Perm_11</td>\n",
              "      <td>Elisa MannesTutora a distância Posso te ajudar...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15685</th>\n",
              "      <td>Documento_40930_Perm_13</td>\n",
              "      <td>O que te faz realmente humano? A escola, com s...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                            id  ... classe\n",
              "14157   Documento_74940_Perm_1  ...      0\n",
              "5362           Documento_47613  ...      1\n",
              "10085  Documento_103572_Perm_6  ...      0\n",
              "21301  Documento_68115_Perm_11  ...      0\n",
              "15685  Documento_40930_Perm_13  ...      0\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwPzTkbnICCD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "482bde83-7877-4e32-8de9-b6af15fbed7b"
      },
      "source": [
        "# Mostra o número de documento de treino.\n",
        "print('Número de documentos de teste: {:,}\\n'.format(dfdados_test.shape[0]))\n",
        "\n",
        "# Informações do DataFrame.\n",
        "print(dfdados_test.info())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Número de documentos de teste: 22,440\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 22440 entries, 0 to 22439\n",
            "Data columns (total 3 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   id         22440 non-null  object\n",
            " 1   documento  22440 non-null  object\n",
            " 2   classe     22440 non-null  int64 \n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 701.2+ KB\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAu53BtuSSfg"
      },
      "source": [
        "### Distribuição das classes\n",
        "\n",
        "O dataset está bem balanceado, o que nos conduz a utilizar acurácia como métrica."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzqyERUfIK2P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "7a2bf841-a2f3-4d3c-aa80-4625143bf4b5"
      },
      "source": [
        "dfdados_test.groupby('classe').count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>documento</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>classe</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>11220</td>\n",
              "      <td>11220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>11220</td>\n",
              "      <td>11220</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           id  documento\n",
              "classe                  \n",
              "0       11220      11220\n",
              "1       11220      11220"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DE0ek_xDKiYC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1d4698d-fd99-4ddd-da62-82ae3ea0aa0e"
      },
      "source": [
        "# Informações do DataFrame.\n",
        "print(dfdados_test.info())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 22440 entries, 0 to 22439\n",
            "Data columns (total 3 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   id         22440 non-null  object\n",
            " 1   documento  22440 non-null  object\n",
            " 2   classe     22440 non-null  int64 \n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 701.2+ KB\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6Mcn1uKSSfh"
      },
      "source": [
        "### Conjunto de dados em Avaliação"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4AtmK5iSSfh",
        "outputId": "39f944d6-c957-455c-f0bb-a142683136a0"
      },
      "source": [
        "# Mostra o resultado dos dados carregados.\n",
        "print(\"Total do conjunto de dados          : {}.\".format(len(dfdados)))\n",
        "print(\"Total do conjunto de dados de teste : {}.\".format(len(documentos_teste)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total do conjunto de dados          : 22440.\n",
            "Total do conjunto de dados de teste : 22440.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bwa6Rts-02-"
      },
      "source": [
        "## 4.6 Treinando o modelo de classificação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AQ-E1QPJqJp"
      },
      "source": [
        "### Otimizador e Agendador de Taxas de Aprendizado/Optimizer & Learning Rate Scheduler\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBi41traJqJp"
      },
      "source": [
        "Agora que temos nosso modelo carregado, precisamos pegar os hiperparâmetros de treinamento no modelo armazenado.\n",
        "\n",
        "Para fins de ajuste fino, os autores recomendam escolher entre os seguintes valores (no Apêndice A.3 do [artigo BERT](https://arxiv.org/pdf/1810.04805.pdf)):\n",
        "\n",
        "> - **Tamanho do lote(Batch size):** 16, 32\n",
        "- **Taxa de aprendizado (Adam):** 5e-5, 3e-5, 2e-5\n",
        "- **Número de épocas:** 2, 3, 4\n",
        "\n",
        "O parâmetro epsilon `eps = 1e-6` é\" um número muito pequeno para impedir qualquer divisão por zero na implementação \"(a partir de [aqui](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)).\n",
        "\n",
        "Você pode encontrar a criação do otimizador do AdamW em `run_glue.py` [aqui](https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L109)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKecsl5K3LR9"
      },
      "source": [
        "### Função carrega otimizador"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajnrdhF63N-r"
      },
      "source": [
        "def carregaOtimizador():\n",
        "\n",
        "  '''\n",
        "    Esta função carrega o otimizador utilizado no agendador de aprendizado.\n",
        "  '''\n",
        "  \n",
        "  # Import das bibliotecas.\n",
        "  from transformers import AdamW\n",
        "\n",
        "  # Nota: AdamW é uma classe da biblioteca huggingface (ao contrário de pytorch).\n",
        "  # Eu acredito que o 'W' significa 'Correção de redução de peso \"\n",
        "  optimizer = AdamW(model.parameters(),\n",
        "                  lr = training_args.learning_rate, # (ou alfa) A taxa de aprendizado a ser usada. - default é 3e-5\n",
        "                  # betas = (0.9, 0.999), # (beta1, beta2) - default é (0.9, 0.999)\n",
        "                    # beta1 é taxa de decaimento exponencial para as estimativas do primeiro momento. \n",
        "                    # beta2 é taxa de decaimento exponencial para as estimativas do segundo momento. Este valor deve ser definido próximo a 1,0 em problemas com gradiente esparso (por exemplo, PNL e problemas de visão de computacional)\n",
        "                  # eps = 1e-6, #  É um número muito pequeno para evitar qualquer divisão por zero na implementação - default é 1e-6.\n",
        "                  # weight_decay = 0.0, # Correção de redução de peso. - default é 0.0\n",
        "                    # A redução da taxa de aprendizagem também pode ser usada com Adam. A taxa de decaimento é atualizada a cada época para a demonstração da regressão logística.\n",
        "                  # correct_bias = True #  Se não deve corrigir o viés(bias) no Adam mudar para False.- default é True\n",
        "                )\n",
        "  \n",
        "  return optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gB7S3ku3UtE"
      },
      "source": [
        "### Carregando otimizador"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLs72DuMODJO"
      },
      "source": [
        "optimizer = carregaOtimizador()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-Aqb27R3cci"
      },
      "source": [
        "### Função carrega agendador"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2MT7UK84srM"
      },
      "source": [
        "A função **get_linear_schedule_with_warmup** cria um agendador com uma taxa de aprendizado que diminua linearmente da taxa de aprendizagem inicial definido no otimizador até 0, após um período de aquecimento durante o qual ele aumenta linearmente de 0 para a taxa de aprendizagem inicial definido no otimizador.\n",
        "\n",
        "Se `num_warmup_steps=0` e `weight_decay=0`(otimizador) não ocorre a etapa de aquecimento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMCaS1VqNr5y"
      },
      "source": [
        "def carregaAgendador():\n",
        "\n",
        "  '''\n",
        "    Esta função carrega o agendador com um taxa de aprendizado que diminua linearmente até 0.\n",
        "  '''\n",
        "\n",
        "  # Import das bibliotecas.\n",
        "  from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "  # O número total de etapas de ajuste fino é [número de lotes] x [número de épocas].\n",
        "  # (Observe que este não é o mesmo que o número de amostras de ajuste fino).\n",
        "  total_etapas = len(documentos_treino) * training_args.num_train_epochs\n",
        "\n",
        "  #Cria o agendador de taxa de aprendizagem.\n",
        "  scheduler = get_linear_schedule_with_warmup(optimizer, # O otimizador para o qual agendar a taxa de aprendizado.\n",
        "                                            num_warmup_steps = 0, # O número de etapas para a fase de aquecimento. Valor default value em run_glue.py\n",
        "                                            num_training_steps = total_etapas) # O número total de etapas de treinamento.\n",
        "\n",
        "\n",
        "  print(\"Total de etapas: {}\".format(total_etapas))\n",
        "\n",
        "  return scheduler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zq-CMoxE3f9m"
      },
      "source": [
        "### Carrega agendador"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gW1WhYvQ3gIy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fdb7931-536c-4fc9-dd63-1fefcf4063b9"
      },
      "source": [
        "scheduler = carregaAgendador()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total de etapas: 39840\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwVBCyXWJqJq"
      },
      "source": [
        "### Função cria lotes inteligentes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTxmZc60JqJq"
      },
      "source": [
        "def cria_lotes_inteligentes(documentos, classes, documentoids, batch_size):\n",
        "    '''\n",
        "    Esta função combina todos os passos para preparar os lotes.\n",
        "    '''\n",
        "    print('Criando Lotes Inteligentes de {:,} amostras com tamanho de lote {:,}...\\n'.format(len(documentos), batch_size))\n",
        "\n",
        "    # ============================\n",
        "    #   Tokenização & Truncamento\n",
        "    # ============================\n",
        "\n",
        "    input_ids_completos = []\n",
        "    \n",
        "    # Tokeniza todas as amostras de treinamento\n",
        "    print('Tokenizando {:,} amostra...'.format(len(classes)))\n",
        "    \n",
        "    # Escolha o intervalo que o progresso será atualizado.\n",
        "    intervalo_atualizacao = obter_intervalo_atualizacao(total_iteracoes=len(classes), numero_atualizacoes=10)\n",
        "    \n",
        "    # Para cada amostra de treinamento...\n",
        "    for documento in documentos:\n",
        "        \n",
        "        # Relatório de progresso\n",
        "        if ((len(input_ids_completos) % intervalo_atualizacao) == 0):\n",
        "            print('  Tokenizado {:,} amostras.'.format(len(input_ids_completos)))\n",
        "\n",
        "        # Tokeniza a amostra.\n",
        "        input_ids = tokenizer.encode(text=documento,                    # Documento a ser codificado.\n",
        "                                    add_special_tokens=True,            # Adiciona os ttokens especiais.\n",
        "                                    max_length=model_args.max_seq_len,  # Tamanho do truncamento!\n",
        "                                    truncation=True,                    # Faz o truncamento!\n",
        "                                    padding=False)                      # Não preenche.\n",
        "                \n",
        "        # Adicione o resultado tokenizado à nossa lista.\n",
        "        input_ids_completos.append(input_ids)\n",
        "        \n",
        "    print('FEITO.')\n",
        "    print('{:>10,} amostras\\n'.format(len(input_ids_completos)))\n",
        "\n",
        "    # =========================\n",
        "    #      Seleciona os Lotes\n",
        "    # =========================    \n",
        "    \n",
        "    # Classifique as duas listas pelo comprimento da sequência de entrada.\n",
        "    amostras = sorted(zip(input_ids_completos, classes, documentoids), key=lambda x: len(x[0]))\n",
        "\n",
        "    print('{:>10,} amostras após classificação\\n'.format(len(amostras)))\n",
        "\n",
        "    import random\n",
        "\n",
        "    # Lista de lotes que iremos construir.\n",
        "    batch_ordered_documentos = []\n",
        "    batch_ordered_classes = []\n",
        "    batch_ordered_documentoids = []\n",
        "\n",
        "    print('Criando lotes de tamanho {:}...'.format(batch_size))\n",
        "\n",
        "    # Escolha um intervalo no qual imprimir atualizações de progresso.\n",
        "    intervalo_atualizacao = obter_intervalo_atualizacao(total_iteracoes=len(amostras), numero_atualizacoes=10)\n",
        "        \n",
        "    # Faça um loop em todas as amostras de entrada ... \n",
        "    while len(amostras) > 0:\n",
        "        \n",
        "        # Mostra o progresso.\n",
        "        if ((len(batch_ordered_documentos) % intervalo_atualizacao) == 0 \\\n",
        "            and not len(batch_ordered_documentos) == 0):\n",
        "            print('  Selecionado {:,} lotes.'.format(len(batch_ordered_documentos)))\n",
        "        \n",
        "        # `to_take` é o tamanho real do nosso lote. Será `batch_size` até\n",
        "        # chegamos ao último lote, que pode ser menor.\n",
        "        to_take = min(batch_size, len(amostras))\n",
        "        \n",
        "        # Escolha um índice aleatório na lista de amostras restantes para começar o nosso lote.\n",
        "        select = random.randint(0, len(amostras) - to_take)\n",
        "\n",
        "        # Selecione um lote contíguo de amostras começando em `select`.\n",
        "        #print (\"Selecionando lote de {:} a {:}\".format(select, select+to_take))\n",
        "        batch = amostras[select:(select + to_take)]\n",
        "\n",
        "        #print(\"Tamanho do lote:\", len(batch))\n",
        "        \n",
        "        # Cada amostra é uma tupla --divida para criar uma lista separada de\n",
        "        # sequências e uma lista de rótulos para este lote.\n",
        "        batch_ordered_documentos.append([s[0] for s in batch])\n",
        "        batch_ordered_classes.append([s[1] for s in batch])\n",
        "        batch_ordered_documentoids.append([s[2] for s in batch])\n",
        "        \n",
        "        # Remova a amostra da lista\n",
        "        del amostras[select:select + to_take]\n",
        "\n",
        "    print('\\n  FEITO - Selecionado {:,} lotes.\\n'.format(len(batch_ordered_documentos)))\n",
        "\n",
        "    # =========================\n",
        "    #        Adicionando o preenchimento\n",
        "    # =========================    \n",
        "\n",
        "    print('Preenchendo sequências dentro de cada lote...')\n",
        "\n",
        "    py_input_ids = []\n",
        "    py_attention_masks = []\n",
        "    py_labels = []\n",
        "    list_documentoids = []\n",
        "\n",
        "    # Para cada lote...\n",
        "    for (batch_input_ids, batch_labels, batch_documentoids) in zip(batch_ordered_documentos, batch_ordered_classes, batch_ordered_documentoids):\n",
        "\n",
        "        # Nova versão do lote, desta vez com sequências preenchidas e agora com\n",
        "        # as máscaras de atenção definidas.\n",
        "        batch_padded_input_ids = []\n",
        "        batch_attention_masks = []\n",
        "                \n",
        "        # Primeiro, encontre a amostra mais longa do lote.\n",
        "        # Observe que as sequências atualmente incluem os tokens especiais!\n",
        "        max_size = max([len(input) for input in batch_input_ids])\n",
        "        \n",
        "        # Para cada entrada neste lote...\n",
        "        for input in batch_input_ids:\n",
        "                        \n",
        "            # Quantos tokens pad precisam ser adicionados\n",
        "            num_pads = max_size - len(input)\n",
        "\n",
        "            # Adiciona `num_pads` do pad token(tokenizer.pad_token_id) até o final da sequência.\n",
        "            padded_input = input + [tokenizer.pad_token_id] * num_pads\n",
        "\n",
        "            # Define a máscara de atenção --é apenas um `1` para cada token real\n",
        "            # e um `0` para cada token de preenchimento(pad).\n",
        "            attention_mask = [1] * len(input) + [0] * num_pads\n",
        "            \n",
        "            # Adiciona o resultado preenchido ao lote.\n",
        "            batch_padded_input_ids.append(padded_input)\n",
        "            batch_attention_masks.append(attention_mask)\n",
        "        \n",
        "        # Nosso lote foi preenchido, portanto, precisamos salvar este lote atualizado.\n",
        "        # Também precisamos que as entradas sejam tensores PyTorch, então faremos isso aqui.\n",
        "        py_input_ids.append(torch.tensor(batch_padded_input_ids))\n",
        "        py_attention_masks.append(torch.tensor(batch_attention_masks))\n",
        "        py_labels.append(torch.tensor(batch_labels))\n",
        "        list_documentoids.append(batch_documentoids)\n",
        "    \n",
        "    # Retorna o conjunto de dados em lotes inteligentes!\n",
        "    return (py_input_ids, py_attention_masks, py_labels, list_documentoids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Thh-qfv5q4tI"
      },
      "source": [
        "### Função de Treinamento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ox8cl_CZDxc-"
      },
      "source": [
        "# Import das bibliotecas\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm as tqdm_notebook\n",
        "\n",
        "def realizaTreinamento(documentos_treino, classes_treino, documentoids_treino, EPOCAS = 4):\n",
        "  \n",
        "  print(\"\\nRealizando Treinamento \")\n",
        "\n",
        "  # Defina o valor da semente em todos os lugares para torná-lo reproduzível.\n",
        "  seed_val = training_args.seed\n",
        "\n",
        "  random.seed(seed_val)\n",
        "  np.random.seed(seed_val)\n",
        "  torch.manual_seed(seed_val)\n",
        "  torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "  # Atualize todos os lotes ʻintervalo_atualizacao`.\n",
        "  intervalo_atualizacao = obter_intervalo_atualizacao(total_iteracoes=len(documentos_treino), numero_atualizacoes=10)\n",
        "\n",
        "  # Medida do tempo total de treinamento.\n",
        "  treinamento_t0 = time.time()\n",
        "\n",
        "  # Limpa o cache da GPU.\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  # Coloque o modelo em modo de treinamento. \n",
        "  model.train()\n",
        "\n",
        "  # Acumula as perdas do treinamento.\n",
        "  train_losses = []\n",
        "\n",
        "  if model_args.use_wandb:\n",
        "    # Log das métricas com wandb.\n",
        "    wandb.watch(model)\n",
        "\n",
        "  # Barra de progresso da época.\n",
        "  epoca_bar = tqdm_notebook(range(training_args.num_train_epochs), desc=f'Épocas', unit=f'épocas')\n",
        "\n",
        "  # Para cada época.\n",
        "  for epoca_i in epoca_bar:\n",
        "    \n",
        "    # ========================================\n",
        "    #               Treinamento\n",
        "    # ========================================\n",
        "    \n",
        "    # Execute uma passada completa sobre o conjunto de treinamento.\n",
        "\n",
        "    # Recupera o lote inteligente\n",
        "    (py_input_ids, py_attention_masks, py_labels, documentoids) = cria_lotes_inteligentes(documentos_treino, classes_treino, documentoids_treino, training_args.per_device_train_batch_size)\n",
        "\n",
        "    # Medida de quanto tempo leva o período de treinamento.\n",
        "    treinamento_epoca_t0 = time.time()\n",
        "\n",
        "    # Acumula as perdas do treinamento da época.\n",
        "    train_epoca_losses = []\n",
        "\n",
        "    # Barras de progresso.    \n",
        "    lote_treino_bar = tqdm_notebook(range(0, len(py_input_ids)), desc=f'Epoca {epoca_i+1}', unit=f'lotes', total=len(py_input_ids) )\n",
        "\n",
        "    # Para cada lote dos dados de treinamento.\n",
        "    for index in lote_treino_bar:      \n",
        "\n",
        "        # Progresso é atualizado a cada lotes, por exemplo, 100 lotes.\n",
        "        if index % intervalo_atualizacao == 0 and not index == 0:            \n",
        "            # Calcula gasto o tempo em minutos.\n",
        "            tempoGasto = formataTempo(time.time() - treinamento_epoca_t0)\n",
        "                        \n",
        "            # Calcule o tempo restante com base em nosso progresso.\n",
        "            passos_por_segundo = (time.time() - treinamento_epoca_t0) / index\n",
        "            segundos_restantes = passos_por_segundo * (len(py_input_ids) - index)\n",
        "            tempoRestante = formataTempo(segundos_restantes)\n",
        "\n",
        "            # Mostra o progresso.\n",
        "            print('  Lote {:>7,}  de  {:>7,}.    Gasto: {:}.  Restante: {:}'.format(index, len(py_input_ids), tempoGasto, tempoRestante))\n",
        "\n",
        "        # Descompacte este lote de treinamento de nosso dataloader.\n",
        "        #\n",
        "        # À medida que descompactamos o lote, também copiaremos cada tensor para a GPU usando o\n",
        "        # o método `to`\n",
        "        #\n",
        "        # `lote` é uma lista contém três tensores pytorch:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "\n",
        "        # Recupera os tensores do lote e copia para a GPU usando o método `to` \n",
        "        d_input_ids = py_input_ids[index].to(device)\n",
        "        d_input_mask = py_attention_masks[index].to(device)\n",
        "        d_labels = py_labels[index].to(device)     \n",
        "        \n",
        "        # Sempre limpe quaisquer gradientes calculados anteriormente antes de realizar um\n",
        "        # passe para trás. PyTorch não faz isso automaticamente porque\n",
        "        # acumular os gradientes é \"conveniente durante o treinamento de RNNs\".\n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Execute um passe para frente (avalie o modelo neste lote de treinamento).\n",
        "        # A documentação para esta função `model` está aqui:\n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        # Ele retorna diferentes números de parâmetros dependendo de quais argumentos\n",
        "        # são fornecidos e quais sinalizadores estão definidos. Para nosso uso aqui, ele retorna\n",
        "        # a perda (porque fornecemos rótulos) e os \"logits\" - o modelo de saídas antes da ativação.     \n",
        "\n",
        "        # last_hidden_state = outputs[0], pooler_output = outputs[1], hidden_states = outputs[2]\n",
        "        outputs = model(d_input_ids, \n",
        "                        token_type_ids=None, \n",
        "                        attention_mask=d_input_mask, \n",
        "                        labels=d_labels)\n",
        "        \n",
        "        # A perda(loss) é retornado em outputs[0] porque fornecemos rótulos(labels))                  \n",
        "        loss = outputs[0]\n",
        "\n",
        "        # E outputs[1] os \"logits\" - o modelo de saídas antes da ativação.\n",
        "        # logits possui duas dimensões, a primeira do lote e a segunda do rótulo da predição                        \n",
        "        # A função `.detach().cpu()` retira da gpu.\n",
        "        logits = outputs[1].detach().cpu()\n",
        "  \n",
        "        # Acumule a perda de treinamento em todos os lotes da época para que possamos\n",
        "        # calcular a perda média no final da época. `loss` é um tensor contendo um único valor.   \n",
        "        # A função '.cpu()' move loss para a cpu.\n",
        "        # A função `.item ()` retorna apenas o valor Python do tensor.\n",
        "        train_epoca_losses.append(loss.cpu().item())\n",
        "        \n",
        "        # Mostra a perda na barra de progresso.\n",
        "        lote_treino_bar.set_postfix(loss=loss.cpu().item())\n",
        "\n",
        "        if model_args.use_wandb:\n",
        "          wandb.log({\"train_batch_loss\": loss.cpu().item()})\n",
        "\n",
        "        # Execute uma passagem para trás para calcular os gradientes.\n",
        "        # Todos os parâmetros do modelo deve ter sido setado para param.requires_grad = False\n",
        "        loss.backward()            \n",
        "\n",
        "        # Corte a norma dos gradientes para 1.0.\n",
        "        # Isso ajuda a evitar o problema de \"gradientes explosivos\".\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "       \n",
        "        # Atualize os parâmetros e dê um passo usando o gradiente calculado.\n",
        "        # O otimizador dita a \"regra de atualização\" - como os parâmetros são\n",
        "        # modificados com base em seus gradientes, taxa de aprendizagem, etc.\n",
        "        optimizer.step()\n",
        "                           \n",
        "        # Atualize a taxa de aprendizagem.\n",
        "        scheduler.step()\n",
        "\n",
        "        del outputs\n",
        "    \n",
        "    # Média da perda do treinamento de todos os lotes da época.\n",
        "    media_train_epoca_loss = np.mean(train_epoca_losses)\n",
        "\n",
        "    # Acumule a perda de treinamento de todas as épocas para calcular a perda média do treinamento.    \n",
        "    train_losses.append(media_train_epoca_loss)\n",
        "\n",
        "    if model_args.use_wandb:\n",
        "      wandb.log({\"media_train_epoca_loss\": media_train_epoca_loss})           \n",
        "        \n",
        "    # Medida de quanto tempo levou essa época.\n",
        "    treinamento_epoca_total = formataTempo(time.time() - treinamento_epoca_t0)\n",
        "\n",
        "    print(\"  Média perda(loss) do treinamento da época : {0:.8f}\".format(media_train_epoca_loss))\n",
        "    print(\"  Tempo de treinamento da época             : {:}\".format(treinamento_epoca_total))    \n",
        "    print(\"  Tempo parcial do treinamento              : {:} (h:mm:ss)\".format(formataTempo(time.time()-treinamento_t0)))\n",
        "\n",
        "    del py_input_ids\n",
        "    del py_attention_masks\n",
        "    del py_labels\n",
        "    del train_epoca_losses\n",
        "    del lote_treino_bar\n",
        "  \n",
        "  # Média da perda do treinamento de todas as épocas.\n",
        "  media_train_loss = np.mean(train_losses)\n",
        "\n",
        "  if model_args.use_wandb:\n",
        "    wandb.log({\"media_train_loss\": media_train_loss})   \n",
        "\n",
        "  print(\"  Média perda(loss) treinamento : {0:.8f}\".format(media_train_loss))\n",
        "\n",
        "  del train_losses\n",
        "  del epoca_bar\n",
        "\n",
        "  print(\"Treinamento completo!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ipCVZG0JqJr"
      },
      "source": [
        "### Execução do Treinamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdJEj_OoJqJr"
      },
      "source": [
        "Estamos prontos para iniciar o treinamento!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5VCQ-psmtLR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "05d880e2577c4024a127698ffb23a089",
            "b60cf5f18cf64247a83aebc59bbae637",
            "363593e3369f4aa29a6619ee336667d2",
            "dfed27c141844087a2a99a24defba724",
            "35ca13ca7fb84b079e400be4e952c3a6",
            "1f8f9501087040e89cfe333da96593d2",
            "45f1f7b9a78649938c1140ea92123076",
            "4ec3d62726f545adb2832d4a2cefaf12",
            "8e12817ee8fb41579e61a7a540ab39fc",
            "806066de79ad44888f25365559da7e7f",
            "ce703f4a2315407781fa0510d3bfa9fc",
            "20726a3880fa49a889388352f9ab2579",
            "5681f7e15fca4166abe8351a7963e323",
            "3393bc00dd3a4207a1eba7ba677bd695",
            "48329bd8b4364dd4b4ada0ae3a314742",
            "f8b7b53cafaf42d98490b17cc6b3f79a",
            "605a5715e0c949f4addc2075a10beb7a",
            "4e82c681c6584c3ea9324347a43bc954",
            "9c2714a2629d498e92682a0a80e3b8a9",
            "a678ecded56f41eda4fc731048528cb2",
            "4b1db2c2ebff4eb38af5838b9b933fd0",
            "ea9c2f669e5d4b7185ed9b5fc76d149c",
            "23794afa54414cd1a0864020752668b5",
            "cb5b671089314d04a30a07b005b4f8d9",
            "91f2ee84ab934faba947f7da420665c9",
            "ba50ab217c734a3a8ade7869940a647e",
            "6903a8fd7b254c498fdda91260d3fcd8",
            "326e476df50b4b02ab9f517ef77e04ba",
            "2be8edfb90144759ad01c5eecd9505b0",
            "22f3a949a6f34e26869b788d8095ced1",
            "b2ee41e865174dab84bfcf85e21b9212",
            "a532bd40cc4a47c09ed3b7160bf89f71",
            "7e51d4909c29465fbb8d3e60feb21014",
            "d18c2a5e9c78404db176640306af4488",
            "6609331566484112bcbd37cff35e3327",
            "e344fb7d0c574d27933dc6c12fc1eacf",
            "56d01ef90987422a8b41bf9a93a13550",
            "05efbb334c464173b78c5d029fc0d1f4",
            "226cfad62541490f991473c4338b3b86",
            "a0d58e5c72154c12a685039692d0c7ad"
          ]
        },
        "outputId": "f8a1455c-8f89-482e-8c57-6a4bec50bc0a"
      },
      "source": [
        "# Registra o tempo inicial.\n",
        "treinamento_t0 = time.time()\n",
        "\n",
        "# Realiza o treinamento.\n",
        "realizaTreinamento(documentos_treino, classes_treino, documentoids_treino, training_args.num_train_epochs)\n",
        "  \n",
        "# Medida de quanto tempo levou a execução do treinamento.\n",
        "treinamento_total = formataTempo(time.time() - treinamento_t0)\n",
        "\n",
        "print(\"  Tempo total treinamento       : {:}\".format(treinamento_total))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Realizando Treinamento \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "05d880e2577c4024a127698ffb23a089",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Épocas', max=4.0, style=ProgressStyle(description_width='…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Criando Lotes Inteligentes de 9,960 amostras com tamanho de lote 4...\n",
            "\n",
            "Tokenizando 9,960 amostra...\n",
            "  Tokenizado 0 amostras.\n",
            "  Tokenizado 1,000 amostras.\n",
            "  Tokenizado 2,000 amostras.\n",
            "  Tokenizado 3,000 amostras.\n",
            "  Tokenizado 4,000 amostras.\n",
            "  Tokenizado 5,000 amostras.\n",
            "  Tokenizado 6,000 amostras.\n",
            "  Tokenizado 7,000 amostras.\n",
            "  Tokenizado 8,000 amostras.\n",
            "  Tokenizado 9,000 amostras.\n",
            "FEITO.\n",
            "     9,960 amostras\n",
            "\n",
            "     9,960 amostras após classificação\n",
            "\n",
            "Criando lotes de tamanho 4...\n",
            "  Selecionado 1,000 lotes.\n",
            "  Selecionado 2,000 lotes.\n",
            "\n",
            "  FEITO - Selecionado 2,490 lotes.\n",
            "\n",
            "Preenchendo sequências dentro de cada lote...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e12817ee8fb41579e61a7a540ab39fc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Epoca 1', max=2490.0, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "  Lote   1,000  de    2,490.    Gasto: 0:02:14.  Restante: 0:03:19\n",
            "  Lote   2,000  de    2,490.    Gasto: 0:04:27.  Restante: 0:01:05\n",
            "\n",
            "  Média perda(loss) do treinamento da época : 0.40507782\n",
            "  Tempo de treinamento da época             : 0:05:31\n",
            "  Tempo parcial do treinamento              : 0:06:05 (h:mm:ss)\n",
            "Criando Lotes Inteligentes de 9,960 amostras com tamanho de lote 4...\n",
            "\n",
            "Tokenizando 9,960 amostra...\n",
            "  Tokenizado 0 amostras.\n",
            "  Tokenizado 1,000 amostras.\n",
            "  Tokenizado 2,000 amostras.\n",
            "  Tokenizado 3,000 amostras.\n",
            "  Tokenizado 4,000 amostras.\n",
            "  Tokenizado 5,000 amostras.\n",
            "  Tokenizado 6,000 amostras.\n",
            "  Tokenizado 7,000 amostras.\n",
            "  Tokenizado 8,000 amostras.\n",
            "  Tokenizado 9,000 amostras.\n",
            "FEITO.\n",
            "     9,960 amostras\n",
            "\n",
            "     9,960 amostras após classificação\n",
            "\n",
            "Criando lotes de tamanho 4...\n",
            "  Selecionado 1,000 lotes.\n",
            "  Selecionado 2,000 lotes.\n",
            "\n",
            "  FEITO - Selecionado 2,490 lotes.\n",
            "\n",
            "Preenchendo sequências dentro de cada lote...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "605a5715e0c949f4addc2075a10beb7a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Epoca 2', max=2490.0, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "  Lote   1,000  de    2,490.    Gasto: 0:02:13.  Restante: 0:03:18\n",
            "  Lote   2,000  de    2,490.    Gasto: 0:04:23.  Restante: 0:01:04\n",
            "\n",
            "  Média perda(loss) do treinamento da época : 0.25911422\n",
            "  Tempo de treinamento da época             : 0:05:28\n",
            "  Tempo parcial do treinamento              : 0:12:04 (h:mm:ss)\n",
            "Criando Lotes Inteligentes de 9,960 amostras com tamanho de lote 4...\n",
            "\n",
            "Tokenizando 9,960 amostra...\n",
            "  Tokenizado 0 amostras.\n",
            "  Tokenizado 1,000 amostras.\n",
            "  Tokenizado 2,000 amostras.\n",
            "  Tokenizado 3,000 amostras.\n",
            "  Tokenizado 4,000 amostras.\n",
            "  Tokenizado 5,000 amostras.\n",
            "  Tokenizado 6,000 amostras.\n",
            "  Tokenizado 7,000 amostras.\n",
            "  Tokenizado 8,000 amostras.\n",
            "  Tokenizado 9,000 amostras.\n",
            "FEITO.\n",
            "     9,960 amostras\n",
            "\n",
            "     9,960 amostras após classificação\n",
            "\n",
            "Criando lotes de tamanho 4...\n",
            "  Selecionado 1,000 lotes.\n",
            "  Selecionado 2,000 lotes.\n",
            "\n",
            "  FEITO - Selecionado 2,490 lotes.\n",
            "\n",
            "Preenchendo sequências dentro de cada lote...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "91f2ee84ab934faba947f7da420665c9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Epoca 3', max=2490.0, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "  Lote   1,000  de    2,490.    Gasto: 0:02:12.  Restante: 0:03:16\n",
            "  Lote   2,000  de    2,490.    Gasto: 0:04:23.  Restante: 0:01:04\n",
            "\n",
            "  Média perda(loss) do treinamento da época : 0.14596666\n",
            "  Tempo de treinamento da época             : 0:05:27\n",
            "  Tempo parcial do treinamento              : 0:18:03 (h:mm:ss)\n",
            "Criando Lotes Inteligentes de 9,960 amostras com tamanho de lote 4...\n",
            "\n",
            "Tokenizando 9,960 amostra...\n",
            "  Tokenizado 0 amostras.\n",
            "  Tokenizado 1,000 amostras.\n",
            "  Tokenizado 2,000 amostras.\n",
            "  Tokenizado 3,000 amostras.\n",
            "  Tokenizado 4,000 amostras.\n",
            "  Tokenizado 5,000 amostras.\n",
            "  Tokenizado 6,000 amostras.\n",
            "  Tokenizado 7,000 amostras.\n",
            "  Tokenizado 8,000 amostras.\n",
            "  Tokenizado 9,000 amostras.\n",
            "FEITO.\n",
            "     9,960 amostras\n",
            "\n",
            "     9,960 amostras após classificação\n",
            "\n",
            "Criando lotes de tamanho 4...\n",
            "  Selecionado 1,000 lotes.\n",
            "  Selecionado 2,000 lotes.\n",
            "\n",
            "  FEITO - Selecionado 2,490 lotes.\n",
            "\n",
            "Preenchendo sequências dentro de cada lote...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7e51d4909c29465fbb8d3e60feb21014",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Epoca 4', max=2490.0, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "  Lote   1,000  de    2,490.    Gasto: 0:02:11.  Restante: 0:03:15\n",
            "  Lote   2,000  de    2,490.    Gasto: 0:04:22.  Restante: 0:01:04\n",
            "\n",
            "  Média perda(loss) do treinamento da época : 0.10466218\n",
            "  Tempo de treinamento da época             : 0:05:26\n",
            "  Tempo parcial do treinamento              : 0:24:01 (h:mm:ss)\n",
            "\n",
            "  Média perda(loss) treinamento : 0.22870522\n",
            "Treinamento completo!\n",
            "  Tempo total treinamento       : 0:24:01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "av-_hPByrUCA"
      },
      "source": [
        "# 5 Avaliação\n",
        "\n",
        "Avaliando o modelo treinado no conjunto de dados de teste."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teXptLZNjszT"
      },
      "source": [
        "## 5.1 Função de Avaliação"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8DIJXnmjw5v"
      },
      "source": [
        "# Import das bibliotecas.\n",
        "import torch\n",
        "from tqdm.notebook import tqdm as tqdm_notebook\n",
        "\n",
        "def realizaAvaliacao(documentos_teste, classes_teste, documentoids_teste):\n",
        "\n",
        "  # Armazena o resultado da avaliação executada\n",
        "  lista_resultado_avaliacao = []\n",
        "\n",
        "  print(\"\\nRealizando Avaliação: {}\")\n",
        "\n",
        "  # Predição no conjunto de teste no modelo.\n",
        "  print('Predizendo rótulos para {:,} documentos de teste...'.format(len(documentos_teste)))\n",
        "\n",
        "  # Use nossa nova função para preparar completamente nosso conjunto de dados.\n",
        "  (py_input_ids, py_attention_masks, py_labels, documentosids) = cria_lotes_inteligentes(documentos_teste, classes_teste, documentoids_teste, training_args.per_device_eval_batch_size)\n",
        "\n",
        "  # Escolha um intervalo para imprimir atualizações de progresso.\n",
        "  intervalo_atualizacao = obter_intervalo_atualizacao(total_iteracoes=len(py_input_ids), numero_atualizacoes=10)\n",
        "\n",
        "  # Coloque o modelo em modo de avaliação.\n",
        "  model.eval()\n",
        "\n",
        "  # Acumula as perdas da avaliação.\n",
        "  test_losses = []\n",
        "\n",
        "  # Acumula os resultados dos testes.\n",
        "  vp = [] # Verdadeiro positivo\n",
        "  vn = [] # Verdadeiro negativo\n",
        "  fp = [] # Falso positivo\n",
        "  fn = [] # Falso negativo\n",
        "\n",
        "  # Barra de progresso dos lotes de teste.\n",
        "  lote_teste_bar = tqdm_notebook(range(0, len(py_input_ids)), desc=f'Lotes ', unit=f'lotes', total=len(py_input_ids))\n",
        "\n",
        "  # Para cada lote dos dados de avaliação(teste).\n",
        "  for index in lote_teste_bar:\n",
        "\n",
        "    # Progresso é atualizado a cada lotes, por exemplo, 100 lotes.\n",
        "    if index % intervalo_atualizacao == 0 and not index == 0:        \n",
        "        # Calcula o tempo gasto em minutos.\n",
        "        tempoGasto = formataTempo(time.time() - avaliacao_t0)\n",
        "        \n",
        "        # Calculate the time tempoRestante based on our progress.\n",
        "        passos_por_segundo = (time.time() - avaliacao_t0) / index\n",
        "        segundos_restantes = passos_por_segundo * (len(py_input_ids) - index)\n",
        "        tempoRestante = formataTempo(segundos_restantes)\n",
        "\n",
        "        # Mostra o progresso.\n",
        "        print('  Lote {:>7,}  de  {:>7,}.    Gasto: {:}.  Restando: {:}'.format(index, len(py_input_ids), tempoGasto, tempoRestante))\n",
        "    \n",
        "    # Copia o lote para a GPU.\n",
        "    d_input_ids = py_input_ids[index].to(device)\n",
        "    d_input_mask = py_attention_masks[index].to(device)\n",
        "    d_labels = py_labels[index].to(device)\n",
        "    d_documentoids = documentosids[index]\n",
        "\n",
        "    # Diga a pytorch para não se preocupar em construir o gráfico de computação durante\n",
        "    # o passe para frente, já que isso só é necessário para backprop (treinamento).\n",
        "    with torch.no_grad():\n",
        "        # Obtenha a saída de \"logits\" pelo modelo. Os \"logits\" são a saída\n",
        "        # valores antes de aplicar uma função de ativação como o softmax.        \n",
        "        # Retorno de model quando ´last_hidden_state=True´ é setado:    \n",
        "        # last_hidden_state = outputs[0], pooler_output = outputs[1], hidden_states = outputs[2]\n",
        "        outputs = model(d_input_ids,\n",
        "                        token_type_ids=None, \n",
        "                        attention_mask=d_input_mask, \n",
        "                        labels=d_labels)\n",
        "        \n",
        "    # A perda(loss) é retornado em outputs[0] porque fornecemos rótulos(labels). \n",
        "    # É útil para comparar com a perda do treinamento, quando é realizado a avaliação entre as épocas de treinamento.\n",
        "    loss = outputs[0]\n",
        "\n",
        "    # E outputs[1] os \"logits\" - o modelo de saídas antes da ativação.\n",
        "    # logits possui duas dimensões, a primeira do lote e a segunda do rótulo da predição\n",
        "    logits = outputs[1]\n",
        "        \n",
        "    # Acumule a perda da avaliação em todos os lotes para que possamos\n",
        "    # calcular a perda média no final. `loss` é um tensor contendo um único valor.\n",
        "    # A função '.cpu()' move loss para a cpu.\n",
        "    # A função `.item ()` retorna apenas o valor Python do tensor.\n",
        "    test_losses.append(loss.cpu().item())\n",
        "\n",
        "    # Recupera o indice do melhor resultado, maior valor dos tensores para coluna(1)\n",
        "    _, classificacao = torch.max(logits, 1)\n",
        "\n",
        "    # Verifica a classificação realizada e o rótulo previsto\n",
        "    vp.append(((classificacao==1) & (d_labels==1)).sum().cpu().item())\n",
        "    vn.append(((classificacao==0) & (d_labels==0)).sum().cpu().item())\n",
        "    fp.append(((classificacao==1) & (d_labels==0)).sum().cpu().item())\n",
        "    fn.append(((classificacao==0) & (d_labels==1)).sum().cpu().item())\n",
        "\n",
        "    # Adiciona o documento de teste, o rótulo e a classificação realizada a lista de resultado\n",
        "    for lote in range(len(d_labels)):\n",
        "                \n",
        "        lista_resultado_avaliacao.append([d_documentoids[lote],\n",
        "                                d_labels[lote].cpu().item(), \n",
        "                                classificacao[lote].cpu().item()])\n",
        "      \n",
        "    del outputs\n",
        "\n",
        "  # Soma as classificações realizadas\n",
        "  vp_s, vn_s, fp_s, fn_s = sum(vp), sum(vn), sum(fp), sum(fn)\n",
        "\n",
        "  # Acurácia indica uma performance geral do modelo. \n",
        "  # Dentre todas as classificações, quantas o modelo classificou corretamente(vp=1 e vn=0).\n",
        "  acc = (vp_s+vn_s)/(vp_s+vn_s+fp_s+fn_s)\n",
        "\n",
        "  # Recall(Revocação) avalia todas as situações da classe Positivo(vp=1) com o valor esperado e quantas estão corretas;\n",
        "  if (vp_s+fn_s) != 0:\n",
        "      rec = (vp_s)/(vp_s+fn_s)\n",
        "  else:\n",
        "      rec = 0\n",
        "\n",
        "  # Precisão avalia as classificações da classe positivo(vp=1 e fp=0) que o modelo fez e quantas estão corretas.\n",
        "  if (vp_s+fp_s) != 0:\n",
        "      pre = (vp_s)/(vp_s+fp_s)\n",
        "  else:\n",
        "      pre = 0  \n",
        "\n",
        "  # F1 é a média harmônica entre precisão e recall.\n",
        "  if (pre + rec) != 0:  \n",
        "    f1 = 2 * ((pre * rec)/(pre + rec))\n",
        "  else:\n",
        "    f1 = 0\n",
        "  \n",
        "  # Média da perda da avaliação\n",
        "  media_test_loss = np.mean(test_losses)\n",
        "\n",
        "  if model_args.use_wandb:\n",
        "    # Log do wandb\n",
        "    wandb.log({\"acuracia\": acc})\n",
        "    wandb.log({\"vp\": vp_s})\n",
        "    wandb.log({\"vn\": vn_s})\n",
        "    wandb.log({\"fp\": fp_s})\n",
        "    wandb.log({\"fn\": fn_s})\n",
        "    wandb.log({\"media_test_loss\": media_test_loss})\n",
        "\n",
        "\n",
        "\n",
        "  del py_input_ids\n",
        "  del py_attention_masks\n",
        "  del py_labels\n",
        "  del test_losses\n",
        "  del lote_teste_bar\n",
        "\n",
        "  return media_test_loss, acc, rec, pre, f1, vp_s, vn_s, fp_s, fn_s, lista_resultado_avaliacao"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8m9VFDtWhRxN"
      },
      "source": [
        "## 5.2 Execução da Avaliação"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9y7MBzY_j_uF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 783,
          "referenced_widgets": [
            "b301d88280c341ff9e75b697fe999806",
            "0e3c82905f5f42819992ab30df785345",
            "7889588c61c94a56b6110c86deacbfc0",
            "6501867cc72946a1aeabf9a807cf2e4f",
            "5b29f0aa86984a3298cf4ec0d99fd1be",
            "c3086a24b4d44815b38e580d8feab2b2",
            "c101539c55554771b253ee62dd8f1ee7",
            "9f525155d22d43c9a5a9bd952652217d"
          ]
        },
        "outputId": "33f9be95-96df-47cb-8771-f186f106f3f4"
      },
      "source": [
        "# Registra o tempo inicial.\n",
        "avaliacao_t0 = time.time()\n",
        "\n",
        "# Realiza a avaliação do modelo.\n",
        "media_test_loss, acc, rec, pre, f1, vp_s, vn_s, fp_s, fn_s, lista_resultado_avaliacao = realizaAvaliacao(documentos_teste, classes_teste, documentoids_teste)\n",
        "\n",
        "print('Avaliação loss           : {:.8f}; Acc: {:.8f}; Rec: {:.8f}; Pre: {:.8f}, F1:{:.8f}, vp: {:3d}; vn: {:3d}; fp: {:3d}; fn: {:3d}'.format( \n",
        "        media_test_loss, acc, rec, pre, f1, vp_s, vn_s, fp_s, fn_s))      \n",
        "\n",
        "print(\"Acurácia                 : {:.8f}\".format(acc))  \n",
        "\n",
        "# Medida de quanto tempo levou a execução do treinamento e avaliação\n",
        "avaliacao_total = formataTempo(time.time() - avaliacao_t0)\n",
        "\n",
        "print(\"Tempo gasto na avaliação : {:}\".format(avaliacao_total))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Realizando Avaliação: {}\n",
            "Predizendo rótulos para 22,440 documentos de teste...\n",
            "Criando Lotes Inteligentes de 22,440 amostras com tamanho de lote 8...\n",
            "\n",
            "Tokenizando 22,440 amostra...\n",
            "  Tokenizado 0 amostras.\n",
            "  Tokenizado 2,000 amostras.\n",
            "  Tokenizado 4,000 amostras.\n",
            "  Tokenizado 6,000 amostras.\n",
            "  Tokenizado 8,000 amostras.\n",
            "  Tokenizado 10,000 amostras.\n",
            "  Tokenizado 12,000 amostras.\n",
            "  Tokenizado 14,000 amostras.\n",
            "  Tokenizado 16,000 amostras.\n",
            "  Tokenizado 18,000 amostras.\n",
            "  Tokenizado 20,000 amostras.\n",
            "  Tokenizado 22,000 amostras.\n",
            "FEITO.\n",
            "    22,440 amostras\n",
            "\n",
            "    22,440 amostras após classificação\n",
            "\n",
            "Criando lotes de tamanho 8...\n",
            "  Selecionado 2,000 lotes.\n",
            "\n",
            "  FEITO - Selecionado 2,805 lotes.\n",
            "\n",
            "Preenchendo sequências dentro de cada lote...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b301d88280c341ff9e75b697fe999806",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Lotes ', max=2805.0, style=ProgressStyle(description_widt…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "  Lote     300  de    2,805.    Gasto: 0:00:47.  Restando: 0:06:30\n",
            "  Lote     600  de    2,805.    Gasto: 0:00:56.  Restando: 0:03:27\n",
            "  Lote     900  de    2,805.    Gasto: 0:01:07.  Restando: 0:02:21\n",
            "  Lote   1,200  de    2,805.    Gasto: 0:01:17.  Restando: 0:01:43\n",
            "  Lote   1,500  de    2,805.    Gasto: 0:01:27.  Restando: 0:01:16\n",
            "  Lote   1,800  de    2,805.    Gasto: 0:01:37.  Restando: 0:00:54\n",
            "  Lote   2,100  de    2,805.    Gasto: 0:01:46.  Restando: 0:00:36\n",
            "  Lote   2,400  de    2,805.    Gasto: 0:01:56.  Restando: 0:00:20\n",
            "  Lote   2,700  de    2,805.    Gasto: 0:02:06.  Restando: 0:00:05\n",
            "\n",
            "Avaliação loss           : 3.07150027; Acc: 0.52508913; Rec: 0.07130125; Pre: 0.77145612, F1:0.13053765, vp: 800; vn: 10983; fp: 237; fn: 10420\n",
            "Acurácia                 : 0.52508913\n",
            "Tempo gasto na avaliação : 0:02:09\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DloA0HIShFzW"
      },
      "source": [
        "## 5.3 Salvando o resultado da classificação"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Is6qvwwh-nJW"
      },
      "source": [
        "def salvaResultadoClassificacao(lista_resultado_avaliacao):\n",
        "\n",
        "  if model_args.salvar_classificacao:\n",
        "\n",
        "    # Import das bibliotecas.\n",
        "    import os\n",
        "    import datetime\n",
        "\n",
        "    # Recupera a hora do sistema.\n",
        "    data_e_hora = datetime.datetime.now()\n",
        "\n",
        "    # Nome arquivo resultado\n",
        "    NOME_ARQUIVO_CLASSIFICACAO = training_args.output_dir + MODELO_BERT + TAMANHO_BERT\n",
        "\n",
        "    # Diretório para salvar o arquivo.\n",
        "    DIRETORIO_CLASSIFICACAO = \"/content/drive/MyDrive/Colab Notebooks/Data/CSTNEWS/validacao_classificacao/holdout/Classificacao/\"\n",
        "\n",
        "    # Verifica se o diretório existe\n",
        "    if not os.path.exists(DIRETORIO_CLASSIFICACAO):  \n",
        "      # Cria o diretório\n",
        "      os.makedirs(DIRETORIO_CLASSIFICACAO)\n",
        "      print('Diretório criado: {}'.format(DIRETORIO_CLASSIFICACAO))\n",
        "    else:\n",
        "      print('Diretório já existe: {}'.format(DIRETORIO_CLASSIFICACAO))\n",
        "\n",
        "    # Nome do arquivo a ser aberto.\n",
        "    NOME_ARQUIVO_CLASSIFICACAO_COMPLETO = DIRETORIO_CLASSIFICACAO + NOME_ARQUIVO_CLASSIFICACAO + \".csv\"\n",
        "\n",
        "    # Gera todo o conteúdo a ser salvo no arquivo\n",
        "    novoConteudo = \"\"        \n",
        "    for resultado in lista_resultado_avaliacao:      \n",
        "      novoConteudo = novoConteudo + data_e_hora.strftime(\"%d/%m/%Y %H:%M\") + \";\" + str(resultado[0]) + \";\" + str(resultado[1]) + \";\" + str(resultado[2]) + \"\\n\"\n",
        "\n",
        "    # Verifica se o arquivo existe.\n",
        "    if os.path.isfile(NOME_ARQUIVO_CLASSIFICACAO_COMPLETO):\n",
        "      print(\"Atualizando arquivo classificação: {}\".format(NOME_ARQUIVO_CLASSIFICACAO_COMPLETO))\n",
        "      # Abre o arquivo para leitura.\n",
        "      arquivo = open(NOME_ARQUIVO_CLASSIFICACAO_COMPLETO,'r')\n",
        "      # Leitura de todas as linhas do arquivo.\n",
        "      conteudo = arquivo.readlines()\n",
        "      # Conteúdo a ser adicionado.\n",
        "      conteudo.append(novoConteudo)\n",
        "\n",
        "      # Abre novamente o arquivo (escrita).\n",
        "      arquivo = open(NOME_ARQUIVO_CLASSIFICACAO_COMPLETO,'w')\n",
        "      # escreva o conteúdo criado anteriormente nele.\n",
        "      arquivo.writelines(conteudo)  \n",
        "      # Fecha o arquivo.\n",
        "      arquivo.close()\n",
        "    else:\n",
        "      print(\"Criando arquivo classificação: {}\".format(NOME_ARQUIVO_CLASSIFICACAO_COMPLETO))\n",
        "      # Abre novamente o arquivo (escrita).\n",
        "      arquivo = open(NOME_ARQUIVO_CLASSIFICACAO_COMPLETO,'w')\n",
        "      arquivo.writelines('data;id;classe;predicao\\n' + novoConteudo)  # escreva o conteúdo criado anteriormente nele.\n",
        "      # Fecha o arquivo.\n",
        "      arquivo.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnu6iKHwAaTb"
      },
      "source": [
        "salvaResultadoClassificacao(lista_resultado_avaliacao)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_otFoSBYBBTD"
      },
      "source": [
        "## 5.4 Salvando o resultado da avaliação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUjV5m1X_TYH"
      },
      "source": [
        "### Salva o resultado da avaliação \n",
        "\n",
        "Salva o resultado da avaliação do conjunto de dados de teste."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oa_cUt7f-9Gb"
      },
      "source": [
        "def salvaResultadoAvaliacao():\n",
        "\n",
        "  if model_args.salvar_avaliacao:\n",
        "\n",
        "    # Import das bibliotecas.\n",
        "    import os\n",
        "    import datetime\n",
        "\n",
        "    # Recupera a hora do sistema.\n",
        "    data_e_hora = datetime.datetime.now()\n",
        "\n",
        "    # Nome arquivo resultado\n",
        "    NOME_ARQUIVO_AVALIACAO = training_args.output_dir + MODELO_BERT + TAMANHO_BERT\n",
        "\n",
        "    # Diretório para salvar o arquivo de resultado.\n",
        "    DIRETORIO_AVALIACAO = \"/content/drive/MyDrive/Colab Notebooks/Data/CSTNEWS/validacao_classificacao/holdout/Avaliacao/\"\n",
        "  \n",
        "    # Verifica se o diretório existe\n",
        "    if not os.path.exists(DIRETORIO_AVALIACAO):  \n",
        "      # Cria o diretório\n",
        "      os.makedirs(DIRETORIO_AVALIACAO)\n",
        "      print('Diretório criado: {}'.format(DIRETORIO_AVALIACAO))\n",
        "    else:\n",
        "      print('Diretório já existe: {}'.format(DIRETORIO_AVALIACAO))\n",
        "\n",
        "    # Nome do arquivo a ser aberto.\n",
        "    NOME_ARQUIVO_AVALIACAO_COMPLETO = DIRETORIO_AVALIACAO + NOME_ARQUIVO_AVALIACAO + \".csv\"\n",
        "\n",
        "    # Conteúdo a ser adicionado.\n",
        "    novoConteudo = NOME_ARQUIVO_AVALIACAO + \";\" + data_e_hora.strftime(\"%d/%m/%Y %H:%M\") + \";\"  + treinamento_total + \";\"  + str(acc) + \";\"  +  str(vp_s) + \";\"  +  str(vn_s) + \";\" +  str(fp_s) + \";\" +  str(fn_s) + \"\\n\"\n",
        "\n",
        "    # Verifica se o arquivo existe.\n",
        "    if os.path.isfile(NOME_ARQUIVO_AVALIACAO_COMPLETO):\n",
        "      print(\"Atualizando arquivo resultado avaliação: {}\".format(NOME_ARQUIVO_AVALIACAO_COMPLETO))\n",
        "      # Abre o arquivo para leitura.\n",
        "      arquivo = open(NOME_ARQUIVO_AVALIACAO_COMPLETO,'r')\n",
        "      # Leitura de todas as linhas do arquivo.\n",
        "      conteudo = arquivo.readlines()\n",
        "      # Conteúdo a ser adicionado.\n",
        "      conteudo.append(novoConteudo)\n",
        "\n",
        "      # Abre novamente o arquivo (escrita).\n",
        "      arquivo = open(NOME_ARQUIVO_AVALIACAO_COMPLETO,'w')\n",
        "      # escreva o conteúdo criado anteriormente nele.\n",
        "      arquivo.writelines(conteudo)  \n",
        "      # Fecha o arquivo.\n",
        "      arquivo.close()\n",
        "    else:\n",
        "      print(\"Criando arquivo resultado avaliação: {}\".format(NOME_ARQUIVO_AVALIACAO_COMPLETO))\n",
        "      # Abre novamente o arquivo (escrita).\n",
        "      arquivo = open(NOME_ARQUIVO_AVALIACAO_COMPLETO,'w')\n",
        "      arquivo.writelines('arquivo;data;tempo;acuracia;vp;vn;fp;fn\\n' + novoConteudo)  # escreva o conteúdo criado anteriormente nele.\n",
        "      # Fecha o arquivo.\n",
        "      arquivo.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aiVdm7P_TlV"
      },
      "source": [
        "salvaResultadoAvaliacao()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUskOwzJmpDB"
      },
      "source": [
        "### Carrega e calcula a média da acurácia das execuções\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-f9-4lCmpDB"
      },
      "source": [
        "def carregaResultadoAvaliacao():\n",
        "\n",
        "  # Import das bibliotecas.\n",
        "  import os\n",
        "  import pandas as pd\n",
        "\n",
        "  # Acumuladores.\n",
        "  somaAcuracia = 0\n",
        "  listaTempo = []\n",
        "  contaExecucoes = 0\n",
        "\n",
        "  # Nome arquivo resultado\n",
        "  NOME_ARQUIVO_AVALIACAO = training_args.output_dir + MODELO_BERT + TAMANHO_BERT\n",
        "\n",
        "  # Diretório para salvar o arquivo.\n",
        "  DIRETORIO_AVALIACAO = \"/content/drive/MyDrive/Colab Notebooks/Data/CSTNEWS/validacao_classificacao/holdout/Avaliacao/\"\n",
        "\n",
        "  # Verifica se o diretório dos resultados existem.\n",
        "  if os.path.exists(DIRETORIO_AVALIACAO):\n",
        "    # Nome do arquivo mais o caminho\n",
        "    NOME_ARQUIVO_AVALIACAO_COMPLETO = DIRETORIO_AVALIACAO + NOME_ARQUIVO_AVALIACAO + \".csv\"\n",
        "    # Verifica se o arquivo existe.\n",
        "    if os.path.isfile(NOME_ARQUIVO_AVALIACAO_COMPLETO):\n",
        "      # Carrega os dados do arquivo  \n",
        "      dados = pd.read_csv(NOME_ARQUIVO_AVALIACAO_COMPLETO, sep=';')\n",
        "\n",
        "      # Mostra os dados do teste da execução.\n",
        "      for index, linha in dados.iterrows():\n",
        "        \n",
        "          # Cálculo das estatísticas\n",
        "          acc = (linha['vp']+linha['vn'])/(linha['vp']+linha['vn']+linha['fp']+linha['fn'])\n",
        "          if (linha['vp']+linha['fn']) != 0:\n",
        "              rec = (linha['vp'])/(linha['vp']+linha['fn'])\n",
        "          else:\n",
        "              rec = 0\n",
        "          if (linha['vp']+linha['fp']) != 0:\n",
        "              pre = (linha['vp'])/(linha['vp']+linha['fp'])\n",
        "          else:  \n",
        "              pre = 0\n",
        "          if (pre + rec) != 0:  \n",
        "              f1 = 2 * ((pre * rec)/(pre + rec))\n",
        "          else:\n",
        "              f1 = 0\n",
        "          qtdeTestes = linha['vp']+linha['vn']+linha['fp']+linha['fn']\n",
        "          print('Arquivo: {}, Data: {}, Tempo:{}, QtdeTeste: {:3d}, Acc: {:.8f}, Rec: {:.8f}, Pre: {:.8f}, F1:{:.8f}, vp: {:4d}; vn: {:4d}; fp: {:4d}; fn: {:4d}'.format(\n",
        "              linha['arquivo'], linha['data'], linha['tempo'], qtdeTestes, acc, rec, pre, f1, linha['vp'], linha['vn'], linha['fp'], linha['fn']))  \n",
        "           \n",
        "          # Guarda o tempo.\n",
        "          listaTempo.append(str(linha['tempo']))\n",
        "\n",
        "          # Procura a maior acurácia.\n",
        "          somaAcuracia = somaAcuracia + acc\n",
        "\n",
        "          # Conta o número de execuções.\n",
        "          contaExecucoes = contaExecucoes + 1\n",
        "\n",
        "      # Mostra a soma da acurácia . \n",
        "      print('Total acurácia                                          : {:.8f}'.format(somaAcuracia))\n",
        "      # Mostra a quantidade de exeucões.\n",
        "      print('Quantidade de execuções                                 : {}'.format(contaExecucoes))  \n",
        "      # Calcula a média.\n",
        "      media = somaAcuracia/contaExecucoes\n",
        "      print('A média da acurácia de {:2d} execuções é                   : {:.8f}'.format(contaExecucoes, media))\n",
        "      print('O tempo gasto na execução do treinamento {:2d} execuções é : {}'.format(contaExecucoes, somaTempo(listaTempo)))\n",
        "      print('A média de tempo de {:2d} execuções é                      : {}'.format(contaExecucoes, mediaTempo(listaTempo)))\n",
        "    else:\n",
        "      print('Arquivo com os resultados não encontrado')    \n",
        "  else:\n",
        "    print('Diretório com os resultados não encontrado')  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITvioKti_sFq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84c0a508-d44c-437c-a1da-9842a4c805e4"
      },
      "source": [
        "carregaResultadoAvaliacao()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Arquivo com os resultados não encontrado\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj0ya60zrm8t"
      },
      "source": [
        "# 6 Finalização"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_zqfWmlIjMc"
      },
      "source": [
        "## 6.1 Salvando o Modelo para o wandb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxLQm2CR-F3m"
      },
      "source": [
        "def salvaModeloWandb():\n",
        "  \n",
        "  if model_args.use_wandb and model_args.salvar_modelo_wandb:\n",
        "  \n",
        "    # Salva o modelo para o wandb    \n",
        "    torch.save(model.state_dict(), os.path.join(wandb.run.dir, 'model_dict.pt'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9U-parMIj00"
      },
      "source": [
        "salvaModeloWandb()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2ZttMYBJqJu"
      },
      "source": [
        "## 6.2 Salvando o Modelo Ajustado\n",
        "\n",
        "Esta primeira célula (obtida de `run_glue.py` [aqui](https://github.com/huggingface/transformers/blob/35ff345fc9df9e777b27903f11fa213e4052595b/examples/run_glue.py#L495)) grava o modelo e o tokenizador no disco."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MDXCrgPJqJu"
      },
      "source": [
        "def salvaModelo():\n",
        "  \n",
        "  if model_args.salvar_modelo:\n",
        "  \n",
        "    # Import de bibliotecas.\n",
        "    import os\n",
        "\n",
        "    # Salvando as melhores práticas: se você usar nomes padrão para o modelo, você pode recarregá-lo usando from_pretrained ()\n",
        "\n",
        "    # Diretório de salvamento do modelo.\n",
        "    output_dir = '/content/model_save/'\n",
        "\n",
        "    # Cria o diretório de saída se necessário.\n",
        "    if not os.path.exists(output_dir):\n",
        "      os.makedirs(output_dir)\n",
        "\n",
        "    print('Salvando o modelo para {}'.format(output_dir))\n",
        "\n",
        "    # Salve um modelo treinado, configuração e tokenizer usando `save_pretrained ()`.\n",
        "    # Eles podem então ser recarregados usando `from_pretrained ()`.\n",
        "    model_to_save = model.module if hasattr(model, 'module') else model  # Cuide do treinamento distribuído/paralelo\n",
        "    model_to_save.save_pretrained(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "    # Boa prática: salve seus argumentos de treinamento junto com o modelo treinado.\n",
        "    torch.save (mode_args, os.path.join (output_dir, 'mode_args.bin'))\n",
        "    torch.save (training_args, os.path.join (output_dir, 'training_args.bin'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ee0YY2PV-Xj7"
      },
      "source": [
        "salvaModelo()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r44zwUNFJqJu"
      },
      "source": [
        "Vamos verificar os tamanhos dos arquivos, por curiosidade."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t60NUZueJqJv"
      },
      "source": [
        "if model_args.salvar_modelo:\n",
        "  !ls -l --block-size=K /content/model_save/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTnm92AaJqJv"
      },
      "source": [
        "O maior arquivo é o peso do modelo, em torno de 416MB o base e 1.25G o large."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nzl0zScjJqJv"
      },
      "source": [
        "if model_args.salvar_modelo:\n",
        "  !ls -l --block-size=M /content/model_save/pytorch_model.bin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OstCHN_JqJv"
      },
      "source": [
        "Para salvar seu modelo nas sessões do Colab Notebook, faça o download no seu computador local ou, idealmente, copie-o no seu Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s19-WOYdJqJv"
      },
      "source": [
        "if model_args.salvar_modelo:\n",
        "\n",
        "  # Importando as bibliotecas.\n",
        "  import os\n",
        "  \n",
        "  # Diretório local de salvamento do modelo.\n",
        "  DIRETORIO_LOCAL_MODELO_AJUSTADO = '/content/modelo_ajustado/'\n",
        "\n",
        "  # Diretório remoto de salvamento do modelo.  \n",
        "  DIRETORIO_REMOTO_MODELO_AJUSTADO = \"/content/drive/MyDrive/Colab Notebooks/Data/CSTNEWS/validacao_classificacao/holdout/modelo/modelo\" + MODELO_BERT + TAMANHO_BERT\n",
        "\n",
        "  # Verifica se o diretório existe\n",
        "  if not os.path.exists(DIRETORIO_REMOTO_MODELO_AJUSTADO):  \n",
        "    # Cria o diretório\n",
        "    os.makedirs(DIRETORIO_REMOTO_MODELO_AJUSTADO)\n",
        "    print('Diretório criado: {}'.format(DIRETORIO_REMOTO_MODELO_AJUSTADO))\n",
        "  else:\n",
        "    print('Diretório já existe: {}'.format(DIRETORIO_REMOTO_MODELO_AJUSTADO))\n",
        "\n",
        "  ## Copia o arquivo do modelo para o diretório no Google Drive.\n",
        "  !cp -r '$DIRETORIO_LOCAL_MODELO_AJUSTADO'* '$DIRETORIO_REMOTO_MODELO_AJUSTADO'\n",
        "\n",
        "  print(\"Modelo copiado!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EUXuiZNpBtL"
      },
      "source": [
        "## 6.3 Tempo final de processamento\n",
        "\n",
        "Tempo processamento:  1:34:52 (h:mm:ss)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H50_GKJwpDha",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fc64a65-0078-4542-ac98-fa6262da3b51"
      },
      "source": [
        " # Pega o tempo atual menos o tempo do início do processamento.\n",
        "finalProcessamento = time.time()\n",
        "tempoTotalProcessamento = formataTempo(finalProcessamento - inicioProcessamento)\n",
        "\n",
        "print(\"\")\n",
        "print(\"  Tempo processamento:  {:} (h:mm:ss)\".format(tempoTotalProcessamento))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  Tempo processamento:  0:29:51 (h:mm:ss)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rD9sTJHMp5Go"
      },
      "source": [
        "Executa o wandb para finalizar a execução anterior"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMRLEXzfp5Q8"
      },
      "source": [
        "if model_args.use_wandb:\n",
        "  \n",
        "    # Importando a biblioteca\n",
        "    import wandb\n",
        "\n",
        "    # Inicializando o registro do experimento\n",
        "    # Na execução só pode existir de um init  para que não gere dois registros no wandb.\n",
        "    wandb.init(project=\"ajustefinocstnews_avaliacaomoodle_v1_c_sb\", name=training_args.output_dir)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}